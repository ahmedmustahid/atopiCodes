{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "837e7231-8754-4cb9-b3ed-ea53b234df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decfeb33-8dee-457e-8445-fd52bcbf3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8484157-452e-4a0a-99da-6ea8e67271a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=(224,224)\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576d94aa-c14c-4978-8419-e3bdd3712f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAtopi = pd.DataFrame(data={\"imagePath\":[str(filePath) for filePath in Path(\"/home/ahmed/work/atopiWork/masks_atopi_skinSegmentation\").rglob(\"*\")]})\n",
    "dfNonAtopi= pd.DataFrame(data={\"imagePath\":[str(filePath) for filePath in Path(\"/home/ahmed/work/atopiWork/masks_NonAtopiImages_augmented\").rglob(\"*\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a05f01c-037e-4b5c-a042-fdf465bebc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAtopi[\"label\"] = 1\n",
    "dfNonAtopi[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e3b5a5-aeca-4bb5-857f-2bc3a16a64f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagePath</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>391 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             imagePath  label\n",
       "0    /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "1    /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "2    /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "3    /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "4    /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "..                                                 ...    ...\n",
       "386  /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "387  /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "388  /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "389  /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "390  /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "\n",
       "[391 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAtopi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d0f606-edb6-429f-a25f-5b6c2ec87cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagePath</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_NonAtopiImage...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_NonAtopiImage...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_NonAtopiImage...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_NonAtopiImage...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_NonAtopiImage...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_NonAtopiImage...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_NonAtopiImage...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_NonAtopiImage...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_NonAtopiImage...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_NonAtopiImage...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             imagePath  label\n",
       "0    /home/ahmed/work/atopiWork/masks_NonAtopiImage...      0\n",
       "1    /home/ahmed/work/atopiWork/masks_NonAtopiImage...      0\n",
       "2    /home/ahmed/work/atopiWork/masks_NonAtopiImage...      0\n",
       "3    /home/ahmed/work/atopiWork/masks_NonAtopiImage...      0\n",
       "4    /home/ahmed/work/atopiWork/masks_NonAtopiImage...      0\n",
       "..                                                 ...    ...\n",
       "251  /home/ahmed/work/atopiWork/masks_NonAtopiImage...      0\n",
       "252  /home/ahmed/work/atopiWork/masks_NonAtopiImage...      0\n",
       "253  /home/ahmed/work/atopiWork/masks_NonAtopiImage...      0\n",
       "254  /home/ahmed/work/atopiWork/masks_NonAtopiImage...      0\n",
       "255  /home/ahmed/work/atopiWork/masks_NonAtopiImage...      0\n",
       "\n",
       "[256 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNonAtopi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d14d37-a633-4255-8b8e-8f145882579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAtopi = shuffle(dfAtopi)\n",
    "dfNonAtopi = shuffle(dfNonAtopi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22650be3-d149-4d7c-8ec7-19042cbc47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAtopiBalanced = dfAtopi[:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e2e947f-d963-42e5-ab1e-d2b0f29ca8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagePath</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>/home/ahmed/work/atopiWork/masks_atopi_skinSeg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             imagePath  label\n",
       "52   /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "216  /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "386  /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "282  /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "195  /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "..                                                 ...    ...\n",
       "154  /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "54   /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "182  /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "254  /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "79   /home/ahmed/work/atopiWork/masks_atopi_skinSeg...      1\n",
       "\n",
       "[256 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAtopiBalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beb7e5d3-832f-45f9-ad0c-9fccc8a41953",
   "metadata": {},
   "outputs": [],
   "source": [
    "newDf = pd.concat([dfAtopiBalanced, dfNonAtopi], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c0c4d96-2937-48e1-b1cd-52d41d95d32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e861d421-72eb-4bc0-9327-5112236d23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1052103c-86a1-4507-b347-04b3bd97e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imFilePaths = newDf[\"imagePath\"].to_numpy()\n",
    "imFileLabels = newDf[\"label\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c2eb0b6-496c-49b7-b439-a4e15f0514a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imFileLabels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daf70cb6-16ee-45af-a273-a3d66e0aee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imFilePaths, imFileLabels = unison_shuffled_copies(imFilePaths, imFileLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35ac7820-c95f-43e0-a81a-e9a084f1907e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imFileLabels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c875a09b-e029-464c-aa75-4011c5c6ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((imFilePaths, imFileLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38c423cd-10f0-4301-ade7-8e1df7b31cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((imFilePaths[:450], imFileLabels[:450]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43124425-72b1-4f0a-8219-3ca6035f2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.data.Dataset.from_tensor_slices((imFilePaths[450:], imFileLabels[450:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cc9b49a-0198-4f80-99bf-638b31b86927",
   "metadata": {},
   "outputs": [],
   "source": [
    "imSizeX, imSizeY = IMG_SIZE\n",
    "def parse_image(filename, label):\n",
    "  #parts = tf.strings.split(filename, os.sep)\n",
    "  #label = parts[-2]\n",
    "\n",
    "  image = tf.io.read_file(filename)\n",
    "  image = tf.io.decode_jpeg(image)\n",
    "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "  image = tf.image.resize(image, [imSizeX, imSizeY])\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89cd5bda-39a0-4380-a043-03f22020d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds =train_ds.map(parse_image, num_parallel_calls=4)\n",
    "val_ds = val_ds.map(parse_image, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "062d13e1-82c5-4d43-a6dc-be58dc031acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(parse_image, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "772c8ba6-3354-4ad4-82f3-4243ef6ad7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_ds.batch(BATCH_SIZE).shuffle(buffer_size=1000)\n",
    "validation_dataset = val_ds.batch(BATCH_SIZE).shuffle(buffer_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89e16210-6849-42a9-bd2a-a1b6df2d0004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "for ims, labs in validation_dataset.take(100):\n",
    "    print(labs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fd4f7dbd-190d-43dc-8a1d-fbfdd512e683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         ...,\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ]],\n",
       " \n",
       "        [[0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         ...,\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ]],\n",
       " \n",
       "        [[0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         ...,\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.9838, 0.9171, 0.8543],\n",
       "         [0.9779, 0.9112, 0.8484],\n",
       "         [0.9833, 0.9166, 0.8539],\n",
       "         ...,\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ]],\n",
       " \n",
       "        [[0.9838, 0.9171, 0.8543],\n",
       "         [0.9782, 0.9115, 0.8487],\n",
       "         [0.9879, 0.9212, 0.8585],\n",
       "         ...,\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ]],\n",
       " \n",
       "        [[0.9838, 0.9171, 0.8543],\n",
       "         [0.9774, 0.9107, 0.848 ],\n",
       "         [0.9849, 0.9182, 0.8555],\n",
       "         ...,\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ],\n",
       "         [0.    , 0.    , 0.    ]]], dtype=float32),\n",
       " 1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.as_numpy_iterator())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7f9f996c-3ecf-4a26-b94e-e797209095d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([num for _, num in dataset.as_numpy_iterator()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3715a793-fe40-4f17-819c-c51316eaa32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9da65def-4bc9-45ba-8f37-4df0c758fdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for arr in td:\n",
    "    #list(td.as_numpy_iterator())[0]\n",
    "    print(i)\n",
    "    i = i +1\n",
    "    #print(len([arr.numpy() for arr in batch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17831e2f-2c67-4115-b5e4-2102d7226260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for arr in td:\n",
    "    #numArr = np.array([arr.numpy() for arr in batch])\n",
    "    if i ==15:\n",
    "        numArr = list([n.numpy() for n in arr])\n",
    "        print(numArr[1].shape)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1438f3e7-d159-4f53-a5a0-07713c6620b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "td1 = td.take(15)\n",
    "td2 = td.skip(15).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6828329c-1981-454b-bf12-e5f27b25a9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n",
      "(32, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in td2.take(1):\n",
    "    print(labels.shape)\n",
    "    print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16421aaf-af76-4dce-bee7-617d34c10252",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "084976e6-0889-47df-929a-e59959c7a29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show(image, label):\n",
    "  plt.figure()\n",
    "  plt.imshow(image)\n",
    "  plt.title(label.numpy())\n",
    "  plt.axis('off')\n",
    "  plt.show()  \n",
    "\n",
    "show(image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e5a2ef0-5fc1-4699-9dd4-942eafac9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    if shuffle:\n",
    "        # Specify seed to always have the same split distribution between runs\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    print(\"train size\", train_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    print(\"val size\", val_size)\n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ba9fdb8c-be3e-4ded-93c9-1b3024017598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 409\n",
      "val size 51\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(ds=td, ds_size=newDf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a596ac57-3ce4-483a-8179-e9a2e78dd63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, n in val_ds.take(51):\n",
    "    print(i.shape)\n",
    "    print(n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19e0ec5a-58dd-4ebd-9efd-013e38adcebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToBatch(datasets, batchSize=32):\n",
    "    #for dataset in datasets:\n",
    "    return dataset.batch(batchSize,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a740f20-d9a9-4a88-9771-1e64a5e78ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=convertToBatch(train_ds)\n",
    "val_ds=convertToBatch(val_ds)\n",
    "test_ds=convertToBatch(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91df4e95-c04e-40f6-a3d2-9ecd2f497d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 224, 224, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(val_ds.as_numpy_iterator())[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a8e6262-9a28-4727-aa0e-465c554d6bd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "/home/ahmed/work/atopiWork/masks_atopi_skinSegmentation/accurate; Is a directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-c4e268bfa14f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# for images, labels in val_ds.take(51):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     print([labels.numpy() for labels in batch])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m110\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print([arr.numpy() for arr in batch])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trainCV/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trainCV/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    784\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trainCV/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2843\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2845\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2846\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2847\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trainCV/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: /home/ahmed/work/atopiWork/masks_atopi_skinSegmentation/accurate; Is a directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "# i=0\n",
    "# for images, labels in val_ds.take(51):\n",
    "#     print([labels.numpy() for labels in batch])\n",
    "for images, labels in td2.take(110):\n",
    "    #print([arr.numpy() for arr in batch])\n",
    "    arr = [label.numpy() for label in labels]\n",
    "    print(len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b1631483-c415-458d-be63-f731c377d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33c130b0-dbdb-46ec-9110-0767a1a97c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "704f0e05-2e80-4ba3-a95e-1f0a5fdd034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d2dcffec-1152-4bba-a9f2-fb6a4a75b7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 7, 7, 1280)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d853cf-fa7f-48c0-829d-69a6c304370f",
   "metadata": {},
   "source": [
    "#### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75f5445b-f352-4443-afd2-2855aeb17754",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33897ef-f268-471b-a0ad-c0defe0d0db0",
   "metadata": {},
   "source": [
    "#### Add a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "41930485-8f31-420a-be83-ebbfe512e2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1280)\n"
     ]
    }
   ],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e97af887-ded3-467d-a854-741541f28be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1)\n"
     ]
    }
   ],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "970bf9c6-03f0-4fd8-a7d6-f081e0a159c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "#x = data_augmentation(inputs)\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "19aaa24f-8238-4675-8810-a30c0c09e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18644fd2-7f11-459e-8933-8f17c8f0b188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bd8fb1fc-dcb9-4b85-a9c1-a99dba0dff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 17ms/step - loss: 0.7057 - accuracy: 0.5246\n"
     ]
    }
   ],
   "source": [
    "initial_epochs = 10\n",
    "\n",
    "loss0, accuracy0 = model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "00c6f59d-8d09-4ea1-b0a3-c04cbbf234c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 2s 21ms/step - loss: 0.7208 - accuracy: 0.4956 - val_loss: 0.6988 - val_accuracy: 0.5246\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.7569 - accuracy: 0.4600 - val_loss: 0.6950 - val_accuracy: 0.5246\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.7131 - accuracy: 0.5089 - val_loss: 0.6937 - val_accuracy: 0.5246\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.7489 - accuracy: 0.4644 - val_loss: 0.6938 - val_accuracy: 0.5246\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.7223 - accuracy: 0.4844 - val_loss: 0.6937 - val_accuracy: 0.5246\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.7297 - accuracy: 0.4867 - val_loss: 0.6953 - val_accuracy: 0.5246\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.7264 - accuracy: 0.5000 - val_loss: 0.6941 - val_accuracy: 0.5246\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.7318 - accuracy: 0.4889 - val_loss: 0.6933 - val_accuracy: 0.5246\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.7024 - accuracy: 0.5311 - val_loss: 0.6927 - val_accuracy: 0.5246\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.7274 - accuracy: 0.5089 - val_loss: 0.6924 - val_accuracy: 0.5246\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83d533b4-3e5e-4f7b-a1ca-a63e13c5284a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d10c2-d43f-45ba-a3dc-1b031166bdff",
   "metadata": {},
   "source": [
    "#### Un-freeze the top layers of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ae0190c3-50ef-4209-9228-38c15ed45be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2984d8a9-8cbf-440a-b415-12ec5cdbf5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  154\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 20\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "755a2c08-c71c-435b-93e5-71a9f8182353",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4b2146f9-0e97-4057-9bef-eef96792c815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "15/15 [==============================] - 3s 58ms/step - loss: 0.7003 - accuracy: 0.4978 - val_loss: 0.6835 - val_accuracy: 0.5246\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.6930 - accuracy: 0.4956 - val_loss: 0.6823 - val_accuracy: 0.5246\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.6973 - accuracy: 0.4956 - val_loss: 0.6798 - val_accuracy: 0.5246\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.6915 - accuracy: 0.5000 - val_loss: 0.6782 - val_accuracy: 0.5246\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.6926 - accuracy: 0.5000 - val_loss: 0.6813 - val_accuracy: 0.5246\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.6898 - accuracy: 0.4978 - val_loss: 0.6766 - val_accuracy: 0.5246\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.6871 - accuracy: 0.5022 - val_loss: 0.6735 - val_accuracy: 0.5246\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.6880 - accuracy: 0.5000 - val_loss: 0.6702 - val_accuracy: 0.5246\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.6882 - accuracy: 0.5111 - val_loss: 0.6708 - val_accuracy: 0.5246\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.6831 - accuracy: 0.5200 - val_loss: 0.6678 - val_accuracy: 0.5246\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.6878 - accuracy: 0.5133 - val_loss: 0.6644 - val_accuracy: 0.5246\n"
     ]
    }
   ],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7eb98ad2-0d34-43bd-a17d-2935574f613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c36a221c-b525-4690-bfbd-10013062d783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292892bb-e8fd-44f4-9f82-4a5fb4061e11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Efficient Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f45ce56e-ffe3-47b7-ac86-79b9beeb38ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16711680/16705208 [==============================] - 1s 0us/step\n",
      "16719872/16705208 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False)\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f375a7d-1328-423c-be0b-3e9d21b767f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape = (224,224,3), name='inputLayer')\n",
    "x = base_model(inputs, training = False)\n",
    "x = layers.GlobalAveragePooling2D(name='poolingLayer')(x)\n",
    "x = layers.Dense(101, name='outputLayer')(x)\n",
    "outputs = layers.Activation(activation=\"softmax\", dtype=tf.float32, name='activationLayer')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name = \"FeatureExtractionModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a747b854-49da-4b4b-aa25-96c103fff341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 inputLayer True float32 <Policy \"float32\">\n",
      "1 efficientnetb0 False float32 <Policy \"float32\">\n",
      "2 poolingLayer True float32 <Policy \"float32\">\n",
      "3 outputLayer True float32 <Policy \"float32\">\n",
      "4 activationLayer True float32 <Policy \"float32\">\n"
     ]
    }
   ],
   "source": [
    "for lnum, layer in enumerate(model.layers):\n",
    "    print(lnum, layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbf47248-2882-4837-953d-0e3217481a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer = tf.keras.optimizers.Adam(),\n",
    "              metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04340c03-e2d8-412e-aca9-8dcee5997f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboard_callback(directory, name):\n",
    "    log_dir = directory + \"/\" + name\n",
    "    t_c = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "    return t_c\n",
    "\n",
    "def model_checkpoint(directory, name):\n",
    "    log_dir = directory + \"/\" + name\n",
    "    m_c = tf.keras.callbacks.ModelCheckpoint(filepath=log_dir,\n",
    "                                             monitor=\"val_accuracy\",\n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=True,\n",
    "                                             verbose=1)\n",
    "    return m_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7210c95c-3014-4345-9eff-14a1682a2912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atopiNonAtopiClassify.ipynb\t preprocessing\n",
      "binaryClassificationAtopy.ipynb  requirements.txt\n",
      "Checkpoints\t\t\t sift\n",
      "createDatasetAtopiTF.ipynb\t Tensorboard\n",
      "melanomaSkinEfficentNet.ipynb\t tfLabelling.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9559c158-d61f-4e57-9102-6e6bdc0f0769",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9256e6eb-1023-4fe6-be70-aa15c8dc2848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7175 - accuracy: 0.5133\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.50000, saving model to Checkpoints/model_1000.ckpt\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.7175 - accuracy: 0.5133 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 2/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7282 - accuracy: 0.5089\n",
      "Epoch 00002: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.7282 - accuracy: 0.5089 - val_loss: 0.6990 - val_accuracy: 0.5000\n",
      "Epoch 3/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7230 - accuracy: 0.4867\n",
      "Epoch 00003: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.7230 - accuracy: 0.4867 - val_loss: 0.6947 - val_accuracy: 0.5000\n",
      "Epoch 4/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7079 - accuracy: 0.5044\n",
      "Epoch 00004: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.7079 - accuracy: 0.5044 - val_loss: 0.7098 - val_accuracy: 0.5000\n",
      "Epoch 5/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7091 - accuracy: 0.4867\n",
      "Epoch 00005: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.7091 - accuracy: 0.4867 - val_loss: 0.6979 - val_accuracy: 0.5000\n",
      "Epoch 6/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.7570 - accuracy: 0.5089\n",
      "Epoch 00006: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.7571 - accuracy: 0.5089 - val_loss: 0.8862 - val_accuracy: 0.5000\n",
      "Epoch 7/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7428 - accuracy: 0.4778\n",
      "Epoch 00007: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.7428 - accuracy: 0.4778 - val_loss: 0.7697 - val_accuracy: 0.5000\n",
      "Epoch 8/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7452 - accuracy: 0.4822\n",
      "Epoch 00008: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7452 - accuracy: 0.4822 - val_loss: 0.7965 - val_accuracy: 0.5000\n",
      "Epoch 9/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7440 - accuracy: 0.5089\n",
      "Epoch 00009: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7440 - accuracy: 0.5089 - val_loss: 0.8986 - val_accuracy: 0.5000\n",
      "Epoch 10/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7665 - accuracy: 0.5089\n",
      "Epoch 00010: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7665 - accuracy: 0.5089 - val_loss: 0.7589 - val_accuracy: 0.5000\n",
      "Epoch 11/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7639 - accuracy: 0.4822\n",
      "Epoch 00011: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7639 - accuracy: 0.4822 - val_loss: 0.7127 - val_accuracy: 0.5000\n",
      "Epoch 12/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7318 - accuracy: 0.4911\n",
      "Epoch 00012: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7318 - accuracy: 0.4911 - val_loss: 0.7204 - val_accuracy: 0.5000\n",
      "Epoch 13/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7603 - accuracy: 0.5178\n",
      "Epoch 00013: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7603 - accuracy: 0.5178 - val_loss: 0.6968 - val_accuracy: 0.5000\n",
      "Epoch 14/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7598 - accuracy: 0.4778\n",
      "Epoch 00014: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7598 - accuracy: 0.4778 - val_loss: 0.8378 - val_accuracy: 0.5000\n",
      "Epoch 15/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7114 - accuracy: 0.5311\n",
      "Epoch 00015: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7114 - accuracy: 0.5311 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 16/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7395 - accuracy: 0.5089\n",
      "Epoch 00016: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7395 - accuracy: 0.5089 - val_loss: 0.8994 - val_accuracy: 0.5000\n",
      "Epoch 17/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.9533 - accuracy: 0.4778\n",
      "Epoch 00017: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.9533 - accuracy: 0.4778 - val_loss: 0.7295 - val_accuracy: 0.5000\n",
      "Epoch 18/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.8031 - accuracy: 0.5400\n",
      "Epoch 00018: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.8031 - accuracy: 0.5400 - val_loss: 0.9041 - val_accuracy: 0.5000\n",
      "Epoch 19/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.8412 - accuracy: 0.5044\n",
      "Epoch 00019: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.8412 - accuracy: 0.5044 - val_loss: 1.0228 - val_accuracy: 0.5000\n",
      "Epoch 20/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.8948 - accuracy: 0.4778\n",
      "Epoch 00020: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.8948 - accuracy: 0.4778 - val_loss: 0.7428 - val_accuracy: 0.5000\n",
      "Epoch 21/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.8296 - accuracy: 0.5133\n",
      "Epoch 00021: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.8296 - accuracy: 0.5133 - val_loss: 0.8862 - val_accuracy: 0.5000\n",
      "Epoch 22/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7830 - accuracy: 0.4822\n",
      "Epoch 00022: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7830 - accuracy: 0.4822 - val_loss: 0.7034 - val_accuracy: 0.5000\n",
      "Epoch 23/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.7455 - accuracy: 0.4955\n",
      "Epoch 00023: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7454 - accuracy: 0.4956 - val_loss: 0.7626 - val_accuracy: 0.5000\n",
      "Epoch 24/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.8344 - accuracy: 0.4956\n",
      "Epoch 00024: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.8344 - accuracy: 0.4956 - val_loss: 1.0112 - val_accuracy: 0.5000\n",
      "Epoch 25/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7873 - accuracy: 0.5133\n",
      "Epoch 00025: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7873 - accuracy: 0.5133 - val_loss: 0.9255 - val_accuracy: 0.5000\n",
      "Epoch 26/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.8849 - accuracy: 0.4867\n",
      "Epoch 00026: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.8849 - accuracy: 0.4867 - val_loss: 0.8669 - val_accuracy: 0.5000\n",
      "Epoch 27/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7720 - accuracy: 0.4822\n",
      "Epoch 00027: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7720 - accuracy: 0.4822 - val_loss: 0.8287 - val_accuracy: 0.5000\n",
      "Epoch 28/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.8245 - accuracy: 0.4867\n",
      "Epoch 00028: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.8245 - accuracy: 0.4867 - val_loss: 0.7433 - val_accuracy: 0.5000\n",
      "Epoch 29/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7165 - accuracy: 0.5133\n",
      "Epoch 00029: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7165 - accuracy: 0.5133 - val_loss: 0.7359 - val_accuracy: 0.5000\n",
      "Epoch 30/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7219 - accuracy: 0.4778\n",
      "Epoch 00030: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7219 - accuracy: 0.4778 - val_loss: 0.6956 - val_accuracy: 0.5000\n",
      "Epoch 31/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7240 - accuracy: 0.4778\n",
      "Epoch 00031: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7240 - accuracy: 0.4778 - val_loss: 0.7023 - val_accuracy: 0.5000\n",
      "Epoch 32/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7154 - accuracy: 0.5000\n",
      "Epoch 00032: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7154 - accuracy: 0.5000 - val_loss: 0.7326 - val_accuracy: 0.5000\n",
      "Epoch 33/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7898 - accuracy: 0.5044\n",
      "Epoch 00033: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7898 - accuracy: 0.5044 - val_loss: 0.7092 - val_accuracy: 0.5000\n",
      "Epoch 34/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7431 - accuracy: 0.5044\n",
      "Epoch 00034: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7431 - accuracy: 0.5044 - val_loss: 0.7000 - val_accuracy: 0.5000\n",
      "Epoch 35/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7181 - accuracy: 0.4600\n",
      "Epoch 00035: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7181 - accuracy: 0.4600 - val_loss: 0.6946 - val_accuracy: 0.5000\n",
      "Epoch 36/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7345 - accuracy: 0.5089\n",
      "Epoch 00036: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7345 - accuracy: 0.5089 - val_loss: 0.7186 - val_accuracy: 0.5000\n",
      "Epoch 37/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7885 - accuracy: 0.4778\n",
      "Epoch 00037: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7885 - accuracy: 0.4778 - val_loss: 0.7437 - val_accuracy: 0.5000\n",
      "Epoch 38/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7732 - accuracy: 0.4867\n",
      "Epoch 00038: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7732 - accuracy: 0.4867 - val_loss: 0.7194 - val_accuracy: 0.5000\n",
      "Epoch 39/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7642 - accuracy: 0.4911\n",
      "Epoch 00039: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7642 - accuracy: 0.4911 - val_loss: 0.8165 - val_accuracy: 0.5000\n",
      "Epoch 40/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7874 - accuracy: 0.4911\n",
      "Epoch 00040: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7874 - accuracy: 0.4911 - val_loss: 0.7132 - val_accuracy: 0.5000\n",
      "Epoch 41/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7459 - accuracy: 0.5000\n",
      "Epoch 00041: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7459 - accuracy: 0.5000 - val_loss: 0.7321 - val_accuracy: 0.5000\n",
      "Epoch 42/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7479 - accuracy: 0.5267\n",
      "Epoch 00042: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7479 - accuracy: 0.5267 - val_loss: 0.6973 - val_accuracy: 0.5000\n",
      "Epoch 43/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7600 - accuracy: 0.5089\n",
      "Epoch 00043: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7600 - accuracy: 0.5089 - val_loss: 0.7276 - val_accuracy: 0.5000\n",
      "Epoch 44/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7408 - accuracy: 0.5000\n",
      "Epoch 00044: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7408 - accuracy: 0.5000 - val_loss: 0.7301 - val_accuracy: 0.5000\n",
      "Epoch 45/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7499 - accuracy: 0.4733\n",
      "Epoch 00045: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7499 - accuracy: 0.4733 - val_loss: 0.6957 - val_accuracy: 0.5000\n",
      "Epoch 46/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7201 - accuracy: 0.5133\n",
      "Epoch 00046: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7201 - accuracy: 0.5133 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 47/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7674 - accuracy: 0.4733\n",
      "Epoch 00047: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7674 - accuracy: 0.4733 - val_loss: 0.8381 - val_accuracy: 0.5000\n",
      "Epoch 48/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7424 - accuracy: 0.4867\n",
      "Epoch 00048: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7424 - accuracy: 0.4867 - val_loss: 0.7309 - val_accuracy: 0.5000\n",
      "Epoch 49/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7546 - accuracy: 0.5000\n",
      "Epoch 00049: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7546 - accuracy: 0.5000 - val_loss: 0.7472 - val_accuracy: 0.5000\n",
      "Epoch 50/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7576 - accuracy: 0.5000\n",
      "Epoch 00050: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7576 - accuracy: 0.5000 - val_loss: 0.7899 - val_accuracy: 0.5000\n",
      "Epoch 51/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.8732 - accuracy: 0.4956\n",
      "Epoch 00051: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.8732 - accuracy: 0.4956 - val_loss: 0.9928 - val_accuracy: 0.5000\n",
      "Epoch 52/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 353.3934 - accuracy: 0.4778\n",
      "Epoch 00052: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 353.3934 - accuracy: 0.4778 - val_loss: 137.4349 - val_accuracy: 0.5000\n",
      "Epoch 53/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 207.3943 - accuracy: 0.4822\n",
      "Epoch 00053: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 207.3943 - accuracy: 0.4822 - val_loss: 0.7554 - val_accuracy: 0.5000\n",
      "Epoch 54/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 31.7781 - accuracy: 0.5044\n",
      "Epoch 00054: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 31.7781 - accuracy: 0.5044 - val_loss: 14.4080 - val_accuracy: 0.5000\n",
      "Epoch 55/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 9.4745 - accuracy: 0.4778\n",
      "Epoch 00055: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 9.4745 - accuracy: 0.4778 - val_loss: 3.8834 - val_accuracy: 0.5000\n",
      "Epoch 56/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 37.5172 - accuracy: 0.5089\n",
      "Epoch 00056: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 37.5172 - accuracy: 0.5089 - val_loss: 1.3233 - val_accuracy: 0.5000\n",
      "Epoch 57/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 12.6696 - accuracy: 0.5311\n",
      "Epoch 00057: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 12.6696 - accuracy: 0.5311 - val_loss: 1.0354 - val_accuracy: 0.5000\n",
      "Epoch 58/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 28.4141 - accuracy: 0.4956\n",
      "Epoch 00058: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 28.4141 - accuracy: 0.4956 - val_loss: 61.5888 - val_accuracy: 0.5000\n",
      "Epoch 59/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 114.0162 - accuracy: 0.5178\n",
      "Epoch 00059: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 114.0162 - accuracy: 0.5178 - val_loss: 72.0740 - val_accuracy: 0.5000\n",
      "Epoch 60/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 94.1706 - accuracy: 0.4867\n",
      "Epoch 00060: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 94.1706 - accuracy: 0.4867 - val_loss: 107.8567 - val_accuracy: 0.5000\n",
      "Epoch 61/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 112.1916 - accuracy: 0.5089\n",
      "Epoch 00061: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 112.1916 - accuracy: 0.5089 - val_loss: 34.6119 - val_accuracy: 0.5000\n",
      "Epoch 62/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 20.9448 - accuracy: 0.5044\n",
      "Epoch 00062: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 20.9448 - accuracy: 0.5044 - val_loss: 30.3399 - val_accuracy: 0.5000\n",
      "Epoch 63/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 19.8968 - accuracy: 0.4867\n",
      "Epoch 00063: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 19.8968 - accuracy: 0.4867 - val_loss: 4.2101 - val_accuracy: 0.5000\n",
      "Epoch 64/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 4.0821 - accuracy: 0.5223\n",
      "Epoch 00064: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 4.0724 - accuracy: 0.5222 - val_loss: 2.7732 - val_accuracy: 0.5000\n",
      "Epoch 65/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 1.5759 - accuracy: 0.5179\n",
      "Epoch 00065: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 1.5724 - accuracy: 0.5178 - val_loss: 1.2045 - val_accuracy: 0.5000\n",
      "Epoch 66/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.9533 - accuracy: 0.5222\n",
      "Epoch 00066: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.9533 - accuracy: 0.5222 - val_loss: 0.8626 - val_accuracy: 0.5000\n",
      "Epoch 67/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7159 - accuracy: 0.4956\n",
      "Epoch 00067: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.7159 - accuracy: 0.4956 - val_loss: 0.7337 - val_accuracy: 0.5000\n",
      "Epoch 68/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7133 - accuracy: 0.5089\n",
      "Epoch 00068: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.7133 - accuracy: 0.5089 - val_loss: 0.6949 - val_accuracy: 0.5000\n",
      "Epoch 69/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7219 - accuracy: 0.5222\n",
      "Epoch 00069: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7219 - accuracy: 0.5222 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 70/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7311 - accuracy: 0.5000\n",
      "Epoch 00070: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7311 - accuracy: 0.5000 - val_loss: 0.6986 - val_accuracy: 0.5000\n",
      "Epoch 71/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7514 - accuracy: 0.4822\n",
      "Epoch 00071: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7514 - accuracy: 0.4822 - val_loss: 0.7950 - val_accuracy: 0.5000\n",
      "Epoch 72/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7571 - accuracy: 0.4911\n",
      "Epoch 00072: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7571 - accuracy: 0.4911 - val_loss: 0.7002 - val_accuracy: 0.5000\n",
      "Epoch 73/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.7093 - accuracy: 0.4732\n",
      "Epoch 00073: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7093 - accuracy: 0.4733 - val_loss: 0.7427 - val_accuracy: 0.5000\n",
      "Epoch 74/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7741 - accuracy: 0.5000\n",
      "Epoch 00074: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7741 - accuracy: 0.5000 - val_loss: 0.8261 - val_accuracy: 0.5000\n",
      "Epoch 75/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7908 - accuracy: 0.4911\n",
      "Epoch 00075: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7908 - accuracy: 0.4911 - val_loss: 0.6982 - val_accuracy: 0.5000\n",
      "Epoch 76/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7333 - accuracy: 0.5089\n",
      "Epoch 00076: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7333 - accuracy: 0.5089 - val_loss: 0.7240 - val_accuracy: 0.5000\n",
      "Epoch 77/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7031 - accuracy: 0.4956\n",
      "Epoch 00077: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.7031 - accuracy: 0.4956 - val_loss: 0.7280 - val_accuracy: 0.5000\n",
      "Epoch 78/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7126 - accuracy: 0.4733\n",
      "Epoch 00078: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7126 - accuracy: 0.4733 - val_loss: 0.6980 - val_accuracy: 0.5000\n",
      "Epoch 79/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7399 - accuracy: 0.4733\n",
      "Epoch 00079: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7399 - accuracy: 0.4733 - val_loss: 0.7055 - val_accuracy: 0.5000\n",
      "Epoch 80/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.7050 - accuracy: 0.5134\n",
      "Epoch 00080: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7050 - accuracy: 0.5133 - val_loss: 0.7347 - val_accuracy: 0.5000\n",
      "Epoch 81/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7086 - accuracy: 0.5044\n",
      "Epoch 00081: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7086 - accuracy: 0.5044 - val_loss: 0.7172 - val_accuracy: 0.5000\n",
      "Epoch 82/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7068 - accuracy: 0.5000\n",
      "Epoch 00082: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7068 - accuracy: 0.5000 - val_loss: 0.7494 - val_accuracy: 0.5000\n",
      "Epoch 83/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7126 - accuracy: 0.4778\n",
      "Epoch 00083: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7126 - accuracy: 0.4778 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 84/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.4778\n",
      "Epoch 00084: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6996 - accuracy: 0.4778 - val_loss: 0.6979 - val_accuracy: 0.5000\n",
      "Epoch 85/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.7145 - accuracy: 0.4955\n",
      "Epoch 00085: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7145 - accuracy: 0.4956 - val_loss: 0.7081 - val_accuracy: 0.5000\n",
      "Epoch 86/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6973 - accuracy: 0.5000\n",
      "Epoch 00086: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6973 - accuracy: 0.5000 - val_loss: 0.7068 - val_accuracy: 0.5000\n",
      "Epoch 87/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6976 - accuracy: 0.4822\n",
      "Epoch 00087: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6976 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 88/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7206 - accuracy: 0.5044\n",
      "Epoch 00088: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7206 - accuracy: 0.5044 - val_loss: 0.7156 - val_accuracy: 0.5000\n",
      "Epoch 89/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7142 - accuracy: 0.5000\n",
      "Epoch 00089: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7142 - accuracy: 0.5000 - val_loss: 0.7141 - val_accuracy: 0.5000\n",
      "Epoch 90/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7354 - accuracy: 0.4867\n",
      "Epoch 00090: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7354 - accuracy: 0.4867 - val_loss: 0.7083 - val_accuracy: 0.5000\n",
      "Epoch 91/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7035 - accuracy: 0.4867\n",
      "Epoch 00091: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7035 - accuracy: 0.4867 - val_loss: 0.6949 - val_accuracy: 0.5000\n",
      "Epoch 92/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7232 - accuracy: 0.5222\n",
      "Epoch 00092: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7232 - accuracy: 0.5222 - val_loss: 0.7115 - val_accuracy: 0.5000\n",
      "Epoch 93/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7158 - accuracy: 0.4689\n",
      "Epoch 00093: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7158 - accuracy: 0.4689 - val_loss: 0.6946 - val_accuracy: 0.5000\n",
      "Epoch 94/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7182 - accuracy: 0.5089\n",
      "Epoch 00094: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7182 - accuracy: 0.5089 - val_loss: 0.7093 - val_accuracy: 0.5000\n",
      "Epoch 95/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7561 - accuracy: 0.4733\n",
      "Epoch 00095: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7561 - accuracy: 0.4733 - val_loss: 0.7494 - val_accuracy: 0.5000\n",
      "Epoch 96/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7266 - accuracy: 0.5133\n",
      "Epoch 00096: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.7266 - accuracy: 0.5133 - val_loss: 0.7524 - val_accuracy: 0.5000\n",
      "Epoch 97/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.7160 - accuracy: 0.4866\n",
      "Epoch 00097: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7161 - accuracy: 0.4867 - val_loss: 0.7223 - val_accuracy: 0.5000\n",
      "Epoch 98/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7192 - accuracy: 0.5178\n",
      "Epoch 00098: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7192 - accuracy: 0.5178 - val_loss: 0.7041 - val_accuracy: 0.5000\n",
      "Epoch 99/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7443 - accuracy: 0.5000\n",
      "Epoch 00099: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7443 - accuracy: 0.5000 - val_loss: 0.7732 - val_accuracy: 0.5000\n",
      "Epoch 100/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7295 - accuracy: 0.4778\n",
      "Epoch 00100: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7295 - accuracy: 0.4778 - val_loss: 0.7038 - val_accuracy: 0.5000\n",
      "Epoch 101/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6997 - accuracy: 0.5000\n",
      "Epoch 00101: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6997 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 102/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7174 - accuracy: 0.4956\n",
      "Epoch 00102: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7174 - accuracy: 0.4956 - val_loss: 0.7632 - val_accuracy: 0.5000\n",
      "Epoch 103/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7655 - accuracy: 0.4733\n",
      "Epoch 00103: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7655 - accuracy: 0.4733 - val_loss: 0.8207 - val_accuracy: 0.5000\n",
      "Epoch 104/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7432 - accuracy: 0.5489\n",
      "Epoch 00104: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7432 - accuracy: 0.5489 - val_loss: 0.8148 - val_accuracy: 0.5000\n",
      "Epoch 105/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7514 - accuracy: 0.5044\n",
      "Epoch 00105: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7514 - accuracy: 0.5044 - val_loss: 0.8015 - val_accuracy: 0.5000\n",
      "Epoch 106/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7361 - accuracy: 0.5000\n",
      "Epoch 00106: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7361 - accuracy: 0.5000 - val_loss: 0.7073 - val_accuracy: 0.5000\n",
      "Epoch 107/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7143 - accuracy: 0.5000\n",
      "Epoch 00107: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7143 - accuracy: 0.5000 - val_loss: 0.7277 - val_accuracy: 0.5000\n",
      "Epoch 108/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6978 - accuracy: 0.5044\n",
      "Epoch 00108: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6978 - accuracy: 0.5044 - val_loss: 0.7205 - val_accuracy: 0.5000\n",
      "Epoch 109/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7162 - accuracy: 0.4600\n",
      "Epoch 00109: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7162 - accuracy: 0.4600 - val_loss: 0.7117 - val_accuracy: 0.5000\n",
      "Epoch 110/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7171 - accuracy: 0.4822\n",
      "Epoch 00110: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7171 - accuracy: 0.4822 - val_loss: 0.7038 - val_accuracy: 0.5000\n",
      "Epoch 111/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7011 - accuracy: 0.4867\n",
      "Epoch 00111: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7011 - accuracy: 0.4867 - val_loss: 0.6975 - val_accuracy: 0.5000\n",
      "Epoch 112/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.5089\n",
      "Epoch 00112: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6952 - accuracy: 0.5089 - val_loss: 0.7096 - val_accuracy: 0.5000\n",
      "Epoch 113/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7317 - accuracy: 0.4822\n",
      "Epoch 00113: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7317 - accuracy: 0.4822 - val_loss: 0.6943 - val_accuracy: 0.5000\n",
      "Epoch 114/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7117 - accuracy: 0.5133\n",
      "Epoch 00114: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7117 - accuracy: 0.5133 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 115/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7009 - accuracy: 0.5133\n",
      "Epoch 00115: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.7009 - accuracy: 0.5133 - val_loss: 0.7336 - val_accuracy: 0.5000\n",
      "Epoch 116/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7052 - accuracy: 0.5089\n",
      "Epoch 00116: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.7052 - accuracy: 0.5089 - val_loss: 0.7304 - val_accuracy: 0.5000\n",
      "Epoch 117/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7142 - accuracy: 0.5089\n",
      "Epoch 00117: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7142 - accuracy: 0.5089 - val_loss: 0.7422 - val_accuracy: 0.5000\n",
      "Epoch 118/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7472 - accuracy: 0.4822\n",
      "Epoch 00118: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7472 - accuracy: 0.4822 - val_loss: 0.7060 - val_accuracy: 0.5000\n",
      "Epoch 119/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7025 - accuracy: 0.5000\n",
      "Epoch 00119: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7025 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 120/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7095 - accuracy: 0.5044\n",
      "Epoch 00120: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7095 - accuracy: 0.5044 - val_loss: 0.7023 - val_accuracy: 0.5000\n",
      "Epoch 121/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7052 - accuracy: 0.4689\n",
      "Epoch 00121: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7052 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 122/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7010 - accuracy: 0.5133\n",
      "Epoch 00122: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7010 - accuracy: 0.5133 - val_loss: 0.6957 - val_accuracy: 0.5000\n",
      "Epoch 123/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7020 - accuracy: 0.4956\n",
      "Epoch 00123: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7020 - accuracy: 0.4956 - val_loss: 0.7027 - val_accuracy: 0.5000\n",
      "Epoch 124/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7088 - accuracy: 0.4956\n",
      "Epoch 00124: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7088 - accuracy: 0.4956 - val_loss: 0.7297 - val_accuracy: 0.5000\n",
      "Epoch 125/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7156 - accuracy: 0.5000\n",
      "Epoch 00125: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.7156 - accuracy: 0.5000 - val_loss: 0.7518 - val_accuracy: 0.5000\n",
      "Epoch 126/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7298 - accuracy: 0.5000\n",
      "Epoch 00126: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7298 - accuracy: 0.5000 - val_loss: 0.7312 - val_accuracy: 0.5000\n",
      "Epoch 127/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7643 - accuracy: 0.4956\n",
      "Epoch 00127: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7643 - accuracy: 0.4956 - val_loss: 0.7085 - val_accuracy: 0.5000\n",
      "Epoch 128/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7259 - accuracy: 0.5000\n",
      "Epoch 00128: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7259 - accuracy: 0.5000 - val_loss: 0.7264 - val_accuracy: 0.5000\n",
      "Epoch 129/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7162 - accuracy: 0.4911\n",
      "Epoch 00129: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7162 - accuracy: 0.4911 - val_loss: 0.7012 - val_accuracy: 0.5000\n",
      "Epoch 130/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6999 - accuracy: 0.5400\n",
      "Epoch 00130: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6999 - accuracy: 0.5400 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 131/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7288 - accuracy: 0.5311\n",
      "Epoch 00131: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7288 - accuracy: 0.5311 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
      "Epoch 132/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6955 - accuracy: 0.5044\n",
      "Epoch 00132: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6955 - accuracy: 0.5044 - val_loss: 0.7100 - val_accuracy: 0.5000\n",
      "Epoch 133/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7075 - accuracy: 0.4956\n",
      "Epoch 00133: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7075 - accuracy: 0.4956 - val_loss: 0.7205 - val_accuracy: 0.5000\n",
      "Epoch 134/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7116 - accuracy: 0.4822\n",
      "Epoch 00134: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7116 - accuracy: 0.4822 - val_loss: 0.7119 - val_accuracy: 0.5000\n",
      "Epoch 135/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7063 - accuracy: 0.5044\n",
      "Epoch 00135: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7063 - accuracy: 0.5044 - val_loss: 0.6976 - val_accuracy: 0.5000\n",
      "Epoch 136/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7136 - accuracy: 0.4778\n",
      "Epoch 00136: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7136 - accuracy: 0.4778 - val_loss: 0.6965 - val_accuracy: 0.5000\n",
      "Epoch 137/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7306 - accuracy: 0.5133\n",
      "Epoch 00137: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7306 - accuracy: 0.5133 - val_loss: 0.6968 - val_accuracy: 0.5000\n",
      "Epoch 138/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7223 - accuracy: 0.5044\n",
      "Epoch 00138: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7223 - accuracy: 0.5044 - val_loss: 0.7353 - val_accuracy: 0.5000\n",
      "Epoch 139/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7085 - accuracy: 0.5044\n",
      "Epoch 00139: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7085 - accuracy: 0.5044 - val_loss: 0.6956 - val_accuracy: 0.5000\n",
      "Epoch 140/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7073 - accuracy: 0.4822\n",
      "Epoch 00140: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7073 - accuracy: 0.4822 - val_loss: 0.6968 - val_accuracy: 0.5000\n",
      "Epoch 141/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7073 - accuracy: 0.5133\n",
      "Epoch 00141: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7073 - accuracy: 0.5133 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 142/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7059 - accuracy: 0.5133\n",
      "Epoch 00142: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7059 - accuracy: 0.5133 - val_loss: 0.7020 - val_accuracy: 0.5000\n",
      "Epoch 143/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6980 - accuracy: 0.4956\n",
      "Epoch 00143: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6980 - accuracy: 0.4956 - val_loss: 0.6967 - val_accuracy: 0.5000\n",
      "Epoch 144/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7008 - accuracy: 0.4733\n",
      "Epoch 00144: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.7008 - accuracy: 0.4733 - val_loss: 0.7053 - val_accuracy: 0.5000\n",
      "Epoch 145/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7034 - accuracy: 0.4689\n",
      "Epoch 00145: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.7034 - accuracy: 0.4689 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 146/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6948 - accuracy: 0.4733\n",
      "Epoch 00146: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6948 - accuracy: 0.4733 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 147/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6978 - accuracy: 0.5089\n",
      "Epoch 00147: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6978 - accuracy: 0.5089 - val_loss: 0.7005 - val_accuracy: 0.5000\n",
      "Epoch 148/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6956 - accuracy: 0.4956\n",
      "Epoch 00148: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6956 - accuracy: 0.4956 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 149/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4867\n",
      "Epoch 00149: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6941 - accuracy: 0.4867 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 150/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6958 - accuracy: 0.4778\n",
      "Epoch 00150: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6958 - accuracy: 0.4778 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 151/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.5089\n",
      "Epoch 00151: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6941 - accuracy: 0.5089 - val_loss: 0.6946 - val_accuracy: 0.5000\n",
      "Epoch 152/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.5089\n",
      "Epoch 00152: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6944 - accuracy: 0.5089 - val_loss: 0.6975 - val_accuracy: 0.5000\n",
      "Epoch 153/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6945 - accuracy: 0.5044\n",
      "Epoch 00153: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6945 - accuracy: 0.5044 - val_loss: 0.6950 - val_accuracy: 0.5000\n",
      "Epoch 154/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6965 - accuracy: 0.4822\n",
      "Epoch 00154: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6965 - accuracy: 0.4822 - val_loss: 0.6947 - val_accuracy: 0.5000\n",
      "Epoch 155/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5089\n",
      "Epoch 00155: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5089 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 156/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.5000\n",
      "Epoch 00156: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6941 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 157/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00157: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 158/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4911\n",
      "Epoch 00158: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6941 - accuracy: 0.4911 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 159/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6953 - accuracy: 0.4778\n",
      "Epoch 00159: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6953 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 160/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6951 - accuracy: 0.5000\n",
      "Epoch 00160: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6951 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 161/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00161: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 162/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4911\n",
      "Epoch 00162: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6940 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 163/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4911\n",
      "Epoch 00163: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4911 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 164/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6945 - accuracy: 0.4644\n",
      "Epoch 00164: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6945 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 165/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00165: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 166/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6945 - accuracy: 0.5000\n",
      "Epoch 00166: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 167/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6951 - accuracy: 0.5044\n",
      "Epoch 00167: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6951 - accuracy: 0.5044 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 168/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.4956\n",
      "Epoch 00168: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6931 - accuracy: 0.4956 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 169/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00169: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 170/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4600\n",
      "Epoch 00170: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 171/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4733\n",
      "Epoch 00171: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 172/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00172: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 173/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4822\n",
      "Epoch 00173: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 174/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6934 - accuracy: 0.4866\n",
      "Epoch 00174: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 175/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.4733\n",
      "Epoch 00175: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6942 - accuracy: 0.4733 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 176/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.5000\n",
      "Epoch 00176: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6942 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 177/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00177: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 178/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00178: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 179/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.4822\n",
      "Epoch 00179: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6939 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 180/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4867\n",
      "Epoch 00180: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 181/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00181: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 182/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.5000\n",
      "Epoch 00182: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 183/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4733\n",
      "Epoch 00183: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6941 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 184/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4600\n",
      "Epoch 00184: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6936 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 185/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6951 - accuracy: 0.4733\n",
      "Epoch 00185: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6951 - accuracy: 0.4733 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 186/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00186: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 187/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4733\n",
      "Epoch 00187: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 188/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.5000\n",
      "Epoch 00188: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6943 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 189/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6949 - accuracy: 0.4956\n",
      "Epoch 00189: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6949 - accuracy: 0.4956 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 190/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5044\n",
      "Epoch 00190: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 191/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00191: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 192/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.5000\n",
      "Epoch 00192: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6940 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 193/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4822\n",
      "Epoch 00193: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 194/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4867\n",
      "Epoch 00194: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 195/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4822\n",
      "Epoch 00195: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4822 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 196/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00196: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 197/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00197: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 198/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.5000\n",
      "Epoch 00198: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 199/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 00199: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 200/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00200: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 201/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00201: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 202/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4778\n",
      "Epoch 00202: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 203/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00203: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 204/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.4956\n",
      "Epoch 00204: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6939 - accuracy: 0.4956 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 205/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00205: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 206/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00206: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 207/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4733\n",
      "Epoch 00207: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 208/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4600\n",
      "Epoch 00208: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 209/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00209: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 210/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4911\n",
      "Epoch 00210: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 211/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00211: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 212/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5089\n",
      "Epoch 00212: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5089 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 213/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4644\n",
      "Epoch 00213: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 214/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00214: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 215/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00215: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 216/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6934 - accuracy: 0.4866\n",
      "Epoch 00216: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 217/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4733\n",
      "Epoch 00217: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 218/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4733\n",
      "Epoch 00218: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 219/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4778\n",
      "Epoch 00219: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 220/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4778\n",
      "Epoch 00220: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 221/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00221: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 222/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00222: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 223/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4911\n",
      "Epoch 00223: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.4911 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 224/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00224: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 225/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4822\n",
      "Epoch 00225: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 226/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00226: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 227/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4911\n",
      "Epoch 00227: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.4911 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 228/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6947 - accuracy: 0.4689\n",
      "Epoch 00228: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6947 - accuracy: 0.4689 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 229/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00229: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 230/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00230: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 231/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4911\n",
      "Epoch 00231: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 232/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4911\n",
      "Epoch 00232: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 233/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00233: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 234/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4778\n",
      "Epoch 00234: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 235/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00235: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 236/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4956\n",
      "Epoch 00236: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 237/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4689\n",
      "Epoch 00237: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 238/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00238: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 239/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4600\n",
      "Epoch 00239: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 240/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6934 - accuracy: 0.4866\n",
      "Epoch 00240: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 241/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00241: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 242/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6935 - accuracy: 0.4777\n",
      "Epoch 00242: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 243/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4778\n",
      "Epoch 00243: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6933 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 244/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4778\n",
      "Epoch 00244: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.6936 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 245/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4822\n",
      "Epoch 00245: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6935 - accuracy: 0.4822 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 246/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00246: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 247/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00247: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 248/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00248: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 249/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00249: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 250/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4778\n",
      "Epoch 00250: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 251/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00251: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 252/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4733\n",
      "Epoch 00252: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 253/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00253: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 254/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4822\n",
      "Epoch 00254: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 255/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4600\n",
      "Epoch 00255: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 256/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00256: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 257/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00257: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 258/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00258: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 259/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4778\n",
      "Epoch 00259: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 260/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4644\n",
      "Epoch 00260: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 261/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00261: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 262/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00262: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 263/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00263: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 264/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4867\n",
      "Epoch 00264: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 265/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00265: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 266/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4733\n",
      "Epoch 00266: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4733 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 267/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4689\n",
      "Epoch 00267: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6941 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 268/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00268: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 269/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00269: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 270/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.5000\n",
      "Epoch 00270: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 271/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4733\n",
      "Epoch 00271: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 272/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4822\n",
      "Epoch 00272: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 273/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4778\n",
      "Epoch 00273: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6936 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 274/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00274: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 275/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6934 - accuracy: 0.4777\n",
      "Epoch 00275: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 276/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4822\n",
      "Epoch 00276: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 277/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4822\n",
      "Epoch 00277: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4822 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 278/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00278: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 279/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4867\n",
      "Epoch 00279: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 280/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4733\n",
      "Epoch 00280: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 281/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4867\n",
      "Epoch 00281: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 282/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4644\n",
      "Epoch 00282: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 283/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4956\n",
      "Epoch 00283: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 284/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00284: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 285/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4689\n",
      "Epoch 00285: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 286/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00286: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 287/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4644\n",
      "Epoch 00287: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 288/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00288: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 289/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4778\n",
      "Epoch 00289: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 290/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4778\n",
      "Epoch 00290: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 291/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00291: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 292/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.5044\n",
      "Epoch 00292: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 293/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00293: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 294/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00294: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 295/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4822\n",
      "Epoch 00295: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 296/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4911\n",
      "Epoch 00296: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 297/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4778\n",
      "Epoch 00297: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6932 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 298/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4867\n",
      "Epoch 00298: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6940 - accuracy: 0.4867 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 299/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4778\n",
      "Epoch 00299: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6938 - accuracy: 0.4778 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 300/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4778\n",
      "Epoch 00300: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 301/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4867\n",
      "Epoch 00301: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 302/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4778\n",
      "Epoch 00302: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4778 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 303/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4644\n",
      "Epoch 00303: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 304/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4956\n",
      "Epoch 00304: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 305/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4867\n",
      "Epoch 00305: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 306/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00306: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 307/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4600\n",
      "Epoch 00307: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 308/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00308: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 309/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4822\n",
      "Epoch 00309: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.6937 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 310/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4778\n",
      "Epoch 00310: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6940 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 311/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4600\n",
      "Epoch 00311: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6941 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 312/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4733\n",
      "Epoch 00312: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 313/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4689\n",
      "Epoch 00313: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 314/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4867\n",
      "Epoch 00314: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4867 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 315/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00315: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 316/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00316: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 317/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4733\n",
      "Epoch 00317: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 318/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.4821\n",
      "Epoch 00318: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 319/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00319: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 320/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4822\n",
      "Epoch 00320: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 321/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00321: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 322/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4956\n",
      "Epoch 00322: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 323/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00323: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 324/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.4777\n",
      "Epoch 00324: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 325/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4689\n",
      "Epoch 00325: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 326/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4778\n",
      "Epoch 00326: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 327/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4867\n",
      "Epoch 00327: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 328/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00328: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 329/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.4822\n",
      "Epoch 00329: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6942 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 330/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4733\n",
      "Epoch 00330: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 331/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00331: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 332/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4911\n",
      "Epoch 00332: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 333/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00333: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 334/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4867\n",
      "Epoch 00334: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 335/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00335: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 336/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00336: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 337/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00337: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 338/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00338: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 339/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00339: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 340/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4644\n",
      "Epoch 00340: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 341/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4778\n",
      "Epoch 00341: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 342/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4733\n",
      "Epoch 00342: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 343/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00343: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 344/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00344: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 345/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00345: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 346/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4822\n",
      "Epoch 00346: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 347/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4778\n",
      "Epoch 00347: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 348/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00348: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 349/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4956\n",
      "Epoch 00349: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 350/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00350: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 351/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00351: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 352/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4644\n",
      "Epoch 00352: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6933 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 353/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00353: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 354/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4867\n",
      "Epoch 00354: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 355/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4867\n",
      "Epoch 00355: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6932 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 356/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4600\n",
      "Epoch 00356: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 357/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00357: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 358/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00358: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 359/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4866\n",
      "Epoch 00359: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6932 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 360/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4778\n",
      "Epoch 00360: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 361/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4689\n",
      "Epoch 00361: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6932 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 362/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00362: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 363/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4956\n",
      "Epoch 00363: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 364/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00364: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 365/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00365: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 366/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4911\n",
      "Epoch 00366: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6932 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 367/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4778\n",
      "Epoch 00367: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.6934 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 368/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4733\n",
      "Epoch 00368: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 369/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00369: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 370/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4866\n",
      "Epoch 00370: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6932 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 371/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4822\n",
      "Epoch 00371: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 372/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00372: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 373/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00373: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 374/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00374: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 375/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4911\n",
      "Epoch 00375: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6935 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 376/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4778\n",
      "Epoch 00376: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 377/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00377: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 378/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4867\n",
      "Epoch 00378: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 379/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4867\n",
      "Epoch 00379: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6932 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 380/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00380: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 381/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4733\n",
      "Epoch 00381: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 382/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4778\n",
      "Epoch 00382: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6934 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 383/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00383: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 384/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4911\n",
      "Epoch 00384: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6932 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 385/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00385: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 386/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00386: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 387/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5044\n",
      "Epoch 00387: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 388/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00388: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 389/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00389: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 390/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4467\n",
      "Epoch 00390: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6935 - accuracy: 0.4467 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 391/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00391: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 392/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00392: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 393/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4822\n",
      "Epoch 00393: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6932 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 394/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00394: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 395/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00395: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 396/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00396: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 397/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4911\n",
      "Epoch 00397: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6932 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 398/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4956\n",
      "Epoch 00398: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 399/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00399: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 400/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00400: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 401/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00401: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 402/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4689\n",
      "Epoch 00402: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 403/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00403: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 404/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4867\n",
      "Epoch 00404: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6932 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 405/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4600\n",
      "Epoch 00405: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.6935 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 406/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00406: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 407/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4733\n",
      "Epoch 00407: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 408/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00408: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 409/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4778\n",
      "Epoch 00409: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6934 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 410/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00410: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 411/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4911\n",
      "Epoch 00411: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6932 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 412/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4689\n",
      "Epoch 00412: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6934 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 413/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00413: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 414/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4867\n",
      "Epoch 00414: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6936 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 415/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00415: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 416/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00416: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 417/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00417: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 418/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00418: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 419/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4956\n",
      "Epoch 00419: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 420/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4689\n",
      "Epoch 00420: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 421/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00421: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 422/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4822\n",
      "Epoch 00422: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 423/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4867\n",
      "Epoch 00423: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 424/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00424: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 425/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4822\n",
      "Epoch 00425: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6932 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 426/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00426: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 427/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00427: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 428/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4956\n",
      "Epoch 00428: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 429/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00429: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 430/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00430: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 431/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4600\n",
      "Epoch 00431: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 432/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00432: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 433/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00433: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 434/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00434: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 435/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00435: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 436/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00436: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 437/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00437: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 438/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4600\n",
      "Epoch 00438: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 439/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00439: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 440/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00440: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 441/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.4821\n",
      "Epoch 00441: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 442/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00442: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 443/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4956\n",
      "Epoch 00443: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 444/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4689\n",
      "Epoch 00444: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6933 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 445/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00445: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 446/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4822\n",
      "Epoch 00446: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6935 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 447/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00447: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 448/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00448: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 449/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00449: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 450/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4822\n",
      "Epoch 00450: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 451/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6935 - accuracy: 0.4821\n",
      "Epoch 00451: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 452/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00452: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 453/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00453: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 454/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00454: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 455/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00455: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 456/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00456: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 457/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00457: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 458/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00458: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 459/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4956\n",
      "Epoch 00459: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 460/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00460: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 461/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00461: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 462/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00462: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 463/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4733\n",
      "Epoch 00463: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 464/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00464: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 465/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00465: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 466/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00466: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 467/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4867\n",
      "Epoch 00467: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 468/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4778\n",
      "Epoch 00468: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 469/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4867\n",
      "Epoch 00469: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 470/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00470: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 471/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4733\n",
      "Epoch 00471: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 472/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4778\n",
      "Epoch 00472: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 473/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00473: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 474/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00474: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 475/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4867\n",
      "Epoch 00475: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 476/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00476: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 477/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4867\n",
      "Epoch 00477: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 478/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4733\n",
      "Epoch 00478: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 479/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4956\n",
      "Epoch 00479: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 480/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00480: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 481/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4911\n",
      "Epoch 00481: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.4911 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 482/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.4556\n",
      "Epoch 00482: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6939 - accuracy: 0.4556 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 483/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00483: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 484/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00484: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 485/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4867\n",
      "Epoch 00485: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 486/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00486: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 487/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4644\n",
      "Epoch 00487: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6932 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 488/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00488: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 489/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4778\n",
      "Epoch 00489: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 490/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4778\n",
      "Epoch 00490: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 491/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00491: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 492/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6935 - accuracy: 0.4554\n",
      "Epoch 00492: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4556 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 493/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00493: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 494/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00494: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 495/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00495: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 496/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00496: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 497/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00497: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 498/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00498: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 499/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4822\n",
      "Epoch 00499: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 500/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00500: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 501/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4778\n",
      "Epoch 00501: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 502/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00502: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 503/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00503: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 504/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00504: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 505/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4822\n",
      "Epoch 00505: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 506/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00506: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 507/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00507: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 508/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00508: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 509/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4911\n",
      "Epoch 00509: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6935 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 510/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4822\n",
      "Epoch 00510: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6936 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 511/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4822\n",
      "Epoch 00511: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6935 - accuracy: 0.4822 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 512/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00512: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 513/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4600\n",
      "Epoch 00513: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 514/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00514: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 515/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4822\n",
      "Epoch 00515: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6940 - accuracy: 0.4822 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 516/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4733\n",
      "Epoch 00516: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4733 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 517/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4733\n",
      "Epoch 00517: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6938 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 518/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00518: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 519/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4778\n",
      "Epoch 00519: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6941 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 520/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4867\n",
      "Epoch 00520: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4867 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 521/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00521: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 522/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4822\n",
      "Epoch 00522: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 523/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00523: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 524/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4867\n",
      "Epoch 00524: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 525/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00525: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 526/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4822\n",
      "Epoch 00526: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 527/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00527: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 528/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.4822\n",
      "Epoch 00528: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6942 - accuracy: 0.4822 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 529/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5133\n",
      "Epoch 00529: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6931 - accuracy: 0.5133 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 530/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.5000\n",
      "Epoch 00530: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 531/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.5000\n",
      "Epoch 00531: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 532/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 00532: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 533/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00533: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 534/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00534: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 535/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00535: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 536/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4956\n",
      "Epoch 00536: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 537/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00537: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 538/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4733\n",
      "Epoch 00538: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 539/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6934 - accuracy: 0.4777\n",
      "Epoch 00539: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 540/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00540: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 541/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.4867\n",
      "Epoch 00541: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6939 - accuracy: 0.4867 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 542/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5044\n",
      "Epoch 00542: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 543/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.5000\n",
      "Epoch 00543: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 544/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5089\n",
      "Epoch 00544: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 545/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00545: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 546/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4822\n",
      "Epoch 00546: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 547/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00547: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 548/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00548: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 549/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5044\n",
      "Epoch 00549: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6934 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 550/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00550: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 551/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00551: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 552/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4778\n",
      "Epoch 00552: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.4778 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 553/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00553: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 554/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00554: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 555/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00555: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 556/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4556\n",
      "Epoch 00556: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4556 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 557/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6935 - accuracy: 0.4955\n",
      "Epoch 00557: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 558/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4822\n",
      "Epoch 00558: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4822 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 559/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4778\n",
      "Epoch 00559: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 560/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00560: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 561/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00561: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 562/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.5000\n",
      "Epoch 00562: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6940 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 563/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00563: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 564/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4778\n",
      "Epoch 00564: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 565/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00565: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 566/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00566: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 567/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.4867\n",
      "Epoch 00567: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6943 - accuracy: 0.4867 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
      "Epoch 568/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6937 - accuracy: 0.5134\n",
      "Epoch 00568: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5133 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 569/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00569: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 570/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4867\n",
      "Epoch 00570: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 571/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00571: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 572/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4867\n",
      "Epoch 00572: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4867 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 573/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.4733\n",
      "Epoch 00573: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6939 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 574/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00574: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 575/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00575: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 576/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6938 - accuracy: 0.4643\n",
      "Epoch 00576: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 577/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4867\n",
      "Epoch 00577: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 578/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00578: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 579/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.4733\n",
      "Epoch 00579: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6939 - accuracy: 0.4733 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 580/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4956\n",
      "Epoch 00580: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 581/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00581: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 582/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00582: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 583/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4867\n",
      "Epoch 00583: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 584/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4867\n",
      "Epoch 00584: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6940 - accuracy: 0.4867 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 585/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5044\n",
      "Epoch 00585: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 586/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4778\n",
      "Epoch 00586: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 587/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00587: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 588/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.5000\n",
      "Epoch 00588: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 589/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5133\n",
      "Epoch 00589: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5133 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 590/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.5000\n",
      "Epoch 00590: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6941 - accuracy: 0.5000 - val_loss: 0.6946 - val_accuracy: 0.5000\n",
      "Epoch 591/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5089\n",
      "Epoch 00591: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.5089 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
      "Epoch 592/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00592: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 593/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.5000\n",
      "Epoch 00593: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6941 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 594/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.4956\n",
      "Epoch 00594: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6939 - accuracy: 0.4956 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 595/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6944 - accuracy: 0.4688\n",
      "Epoch 00595: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6944 - accuracy: 0.4689 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 596/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4911\n",
      "Epoch 00596: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 597/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4911\n",
      "Epoch 00597: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 598/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4689\n",
      "Epoch 00598: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 599/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00599: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 600/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00600: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 601/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6936 - accuracy: 0.4866\n",
      "Epoch 00601: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 602/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4822\n",
      "Epoch 00602: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 603/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4733\n",
      "Epoch 00603: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6935 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 604/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6936 - accuracy: 0.4688\n",
      "Epoch 00604: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 605/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5044\n",
      "Epoch 00605: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 606/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00606: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 607/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00607: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 608/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5133\n",
      "Epoch 00608: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6931 - accuracy: 0.5133 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 609/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00609: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 610/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6950 - accuracy: 0.4554\n",
      "Epoch 00610: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6950 - accuracy: 0.4556 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 611/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00611: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 612/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6948 - accuracy: 0.4822\n",
      "Epoch 00612: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6948 - accuracy: 0.4822 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 613/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.4644\n",
      "Epoch 00613: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6946 - accuracy: 0.4644 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 614/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.4689\n",
      "Epoch 00614: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6943 - accuracy: 0.4689 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 615/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4867\n",
      "Epoch 00615: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4867 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 616/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.5000\n",
      "Epoch 00616: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 617/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4867\n",
      "Epoch 00617: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.4867 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 618/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00618: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 619/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00619: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 620/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4867\n",
      "Epoch 00620: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 621/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4956\n",
      "Epoch 00621: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 622/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00622: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 623/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00623: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 624/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00624: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 625/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00625: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 626/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00626: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 627/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00627: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 628/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4778\n",
      "Epoch 00628: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6940 - accuracy: 0.4778 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 629/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4911\n",
      "Epoch 00629: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 630/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4733\n",
      "Epoch 00630: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 631/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4733\n",
      "Epoch 00631: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 632/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4867\n",
      "Epoch 00632: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 633/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4911\n",
      "Epoch 00633: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6938 - accuracy: 0.4911 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 634/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00634: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 635/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4822\n",
      "Epoch 00635: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6938 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 636/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4822\n",
      "Epoch 00636: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 637/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00637: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 638/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4822\n",
      "Epoch 00638: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6938 - accuracy: 0.4822 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 639/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4911\n",
      "Epoch 00639: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 640/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4733\n",
      "Epoch 00640: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 641/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00641: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 642/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.5000\n",
      "Epoch 00642: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6943 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 643/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4956\n",
      "Epoch 00643: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.4956 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 644/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4822\n",
      "Epoch 00644: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6941 - accuracy: 0.4822 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 645/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00645: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 646/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4778\n",
      "Epoch 00646: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 647/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00647: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 648/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4867\n",
      "Epoch 00648: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 649/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4778\n",
      "Epoch 00649: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 650/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4511\n",
      "Epoch 00650: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4511 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 651/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00651: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 652/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.4778\n",
      "Epoch 00652: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6939 - accuracy: 0.4778 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 653/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00653: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 654/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5089\n",
      "Epoch 00654: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 655/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00655: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 656/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00656: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 657/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4867\n",
      "Epoch 00657: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.4867 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 658/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00658: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 659/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4778\n",
      "Epoch 00659: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6941 - accuracy: 0.4778 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 660/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00660: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 661/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6937 - accuracy: 0.4821\n",
      "Epoch 00661: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.4822 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 662/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4956\n",
      "Epoch 00662: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 663/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00663: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 664/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.4778\n",
      "Epoch 00664: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6939 - accuracy: 0.4778 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 665/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4822\n",
      "Epoch 00665: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4822 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 666/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00666: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 667/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00667: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 668/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4822\n",
      "Epoch 00668: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6941 - accuracy: 0.4822 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 669/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00669: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 670/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.5000\n",
      "Epoch 00670: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6946 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 671/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00671: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 672/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4911\n",
      "Epoch 00672: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6938 - accuracy: 0.4911 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 673/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00673: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 674/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4867\n",
      "Epoch 00674: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 675/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4778\n",
      "Epoch 00675: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 676/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.5000\n",
      "Epoch 00676: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6938 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 677/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4956\n",
      "Epoch 00677: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.4956 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 678/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00678: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 679/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5044\n",
      "Epoch 00679: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 680/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00680: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 681/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00681: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 682/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.5000\n",
      "Epoch 00682: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6942 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 683/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4822\n",
      "Epoch 00683: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6938 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 684/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.4822\n",
      "Epoch 00684: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6943 - accuracy: 0.4822 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 685/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5089\n",
      "Epoch 00685: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.5089 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 686/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00686: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 687/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00687: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 688/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4689\n",
      "Epoch 00688: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 689/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.5000\n",
      "Epoch 00689: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 690/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6934 - accuracy: 0.4955\n",
      "Epoch 00690: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 691/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4822\n",
      "Epoch 00691: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 692/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00692: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 693/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6937 - accuracy: 0.4732\n",
      "Epoch 00693: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.4733 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 694/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4733\n",
      "Epoch 00694: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 695/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4911\n",
      "Epoch 00695: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.4911 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 696/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.5000\n",
      "Epoch 00696: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 697/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4778\n",
      "Epoch 00697: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 698/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6947 - accuracy: 0.5000\n",
      "Epoch 00698: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6945 - val_accuracy: 0.5000\n",
      "Epoch 699/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4867\n",
      "Epoch 00699: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 700/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4733\n",
      "Epoch 00700: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6940 - accuracy: 0.4733 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 701/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.5000\n",
      "Epoch 00701: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6946 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 702/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.5044\n",
      "Epoch 00702: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6944 - accuracy: 0.5044 - val_loss: 0.6951 - val_accuracy: 0.5000\n",
      "Epoch 703/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.5000\n",
      "Epoch 00703: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6942 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 704/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00704: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 705/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4822\n",
      "Epoch 00705: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.4822 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 706/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 00706: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 707/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4600\n",
      "Epoch 00707: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 708/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00708: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 709/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4911\n",
      "Epoch 00709: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.4911 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 710/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00710: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 711/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4644\n",
      "Epoch 00711: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 712/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00712: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 713/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4689\n",
      "Epoch 00713: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 714/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4689\n",
      "Epoch 00714: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 715/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00715: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 716/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4867\n",
      "Epoch 00716: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 717/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4956\n",
      "Epoch 00717: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 718/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.5000\n",
      "Epoch 00718: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6941 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 719/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4778\n",
      "Epoch 00719: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6936 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 720/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00720: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 721/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00721: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 722/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00722: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 723/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00723: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 724/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4822\n",
      "Epoch 00724: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6941 - accuracy: 0.4822 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 725/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4911\n",
      "Epoch 00725: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.4911 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 726/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00726: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 727/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4867\n",
      "Epoch 00727: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 728/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00728: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 729/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00729: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 730/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00730: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 731/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4911\n",
      "Epoch 00731: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4911 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 732/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4733\n",
      "Epoch 00732: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 733/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.5000\n",
      "Epoch 00733: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6941 - accuracy: 0.5000 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 734/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.4911\n",
      "Epoch 00734: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6942 - accuracy: 0.4911 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 735/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.4778\n",
      "Epoch 00735: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6952 - accuracy: 0.4778 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 736/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4911\n",
      "Epoch 00736: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 737/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00737: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 738/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4778\n",
      "Epoch 00738: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6938 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 739/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00739: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 740/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4867\n",
      "Epoch 00740: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6940 - accuracy: 0.4867 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 741/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00741: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 742/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6936 - accuracy: 0.4955\n",
      "Epoch 00742: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 743/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4867\n",
      "Epoch 00743: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 744/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00744: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 745/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00745: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 746/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00746: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 747/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4733\n",
      "Epoch 00747: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 748/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4822\n",
      "Epoch 00748: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6941 - accuracy: 0.4822 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 749/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00749: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 750/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4956\n",
      "Epoch 00750: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 751/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00751: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 752/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.5000\n",
      "Epoch 00752: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6938 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 753/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4822\n",
      "Epoch 00753: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 754/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4733\n",
      "Epoch 00754: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 755/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4689\n",
      "Epoch 00755: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 756/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4822\n",
      "Epoch 00756: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6940 - accuracy: 0.4822 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 757/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4822\n",
      "Epoch 00757: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 758/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4644\n",
      "Epoch 00758: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6938 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 759/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00759: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 760/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.4733\n",
      "Epoch 00760: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6944 - accuracy: 0.4733 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 761/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.4822\n",
      "Epoch 00761: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6942 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 762/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.5044\n",
      "Epoch 00762: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.5044 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 763/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4911\n",
      "Epoch 00763: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6940 - accuracy: 0.4911 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 764/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4867\n",
      "Epoch 00764: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6938 - accuracy: 0.4867 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 765/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00765: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 766/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4867\n",
      "Epoch 00766: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 767/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4733\n",
      "Epoch 00767: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4733 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 768/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.5000\n",
      "Epoch 00768: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 769/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.5000\n",
      "Epoch 00769: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6942 - accuracy: 0.5000 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 770/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.5000\n",
      "Epoch 00770: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6940 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 771/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.5000\n",
      "Epoch 00771: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6940 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 772/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5133\n",
      "Epoch 00772: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.5133 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 773/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00773: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 774/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4867\n",
      "Epoch 00774: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6938 - accuracy: 0.4867 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 775/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00775: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 776/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00776: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 777/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4511\n",
      "Epoch 00777: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4511 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 778/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00778: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 779/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00779: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 780/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.5000\n",
      "Epoch 00780: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6942 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 781/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4956\n",
      "Epoch 00781: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 782/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6945 - accuracy: 0.4733\n",
      "Epoch 00782: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6945 - accuracy: 0.4733 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 783/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4644\n",
      "Epoch 00783: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6935 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 784/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4778\n",
      "Epoch 00784: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 785/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00785: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 786/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.5000\n",
      "Epoch 00786: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 787/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4911\n",
      "Epoch 00787: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6938 - accuracy: 0.4911 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 788/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5044\n",
      "Epoch 00788: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 789/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4733\n",
      "Epoch 00789: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6938 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 790/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4911\n",
      "Epoch 00790: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6938 - accuracy: 0.4911 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 791/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.4911\n",
      "Epoch 00791: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6939 - accuracy: 0.4911 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 792/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.5000\n",
      "Epoch 00792: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6944 - accuracy: 0.5000 - val_loss: 0.6952 - val_accuracy: 0.5000\n",
      "Epoch 793/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5133\n",
      "Epoch 00793: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.5133 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 794/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.5000\n",
      "Epoch 00794: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 795/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5044\n",
      "Epoch 00795: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 796/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4867\n",
      "Epoch 00796: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 797/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4689\n",
      "Epoch 00797: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 798/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4867\n",
      "Epoch 00798: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4867 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 799/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.4822\n",
      "Epoch 00799: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6942 - accuracy: 0.4822 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 800/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4867\n",
      "Epoch 00800: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 801/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4778\n",
      "Epoch 00801: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4778 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 802/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00802: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 803/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4822\n",
      "Epoch 00803: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 804/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4956\n",
      "Epoch 00804: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 805/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4956\n",
      "Epoch 00805: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6937 - accuracy: 0.4956 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 806/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 00806: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 807/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4822\n",
      "Epoch 00807: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 808/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6945 - accuracy: 0.5000\n",
      "Epoch 00808: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 809/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.5000\n",
      "Epoch 00809: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6940 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 810/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4822\n",
      "Epoch 00810: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6935 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 811/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6940 - accuracy: 0.5000\n",
      "Epoch 00811: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6940 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 812/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00812: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 813/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.4822\n",
      "Epoch 00813: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6944 - accuracy: 0.4822 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 814/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4822\n",
      "Epoch 00814: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.4822 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 815/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4822\n",
      "Epoch 00815: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6941 - accuracy: 0.4822 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 816/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4911\n",
      "Epoch 00816: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6940 - accuracy: 0.4911 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 817/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4956\n",
      "Epoch 00817: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4956 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 818/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.5000\n",
      "Epoch 00818: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6944 - accuracy: 0.5000 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 819/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6947 - accuracy: 0.4689\n",
      "Epoch 00819: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6947 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 820/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.5000\n",
      "Epoch 00820: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6941 - accuracy: 0.5000 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 821/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5089\n",
      "Epoch 00821: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5089 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 822/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5044\n",
      "Epoch 00822: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 823/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00823: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 824/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4867\n",
      "Epoch 00824: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 825/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4822\n",
      "Epoch 00825: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.4822 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 826/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4600\n",
      "Epoch 00826: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4600 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 827/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00827: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 828/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.5000\n",
      "Epoch 00828: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6943 - accuracy: 0.5000 - val_loss: 0.6948 - val_accuracy: 0.5000\n",
      "Epoch 829/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6938 - accuracy: 0.4866\n",
      "Epoch 00829: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6938 - accuracy: 0.4867 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 830/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00830: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 831/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00831: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 832/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4867\n",
      "Epoch 00832: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6937 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 833/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5044\n",
      "Epoch 00833: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 834/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4733\n",
      "Epoch 00834: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 835/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4911\n",
      "Epoch 00835: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 836/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00836: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 837/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4911\n",
      "Epoch 00837: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 838/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4822\n",
      "Epoch 00838: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6940 - accuracy: 0.4822 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 839/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4911\n",
      "Epoch 00839: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6938 - accuracy: 0.4911 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 840/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00840: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 841/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00841: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 842/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4778\n",
      "Epoch 00842: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 843/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00843: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 844/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4867\n",
      "Epoch 00844: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 845/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00845: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 846/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4911\n",
      "Epoch 00846: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6940 - accuracy: 0.4911 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 847/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00847: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 848/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4822\n",
      "Epoch 00848: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.4822 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 849/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6935 - accuracy: 0.4643\n",
      "Epoch 00849: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6935 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 850/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4733\n",
      "Epoch 00850: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6936 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 851/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4911\n",
      "Epoch 00851: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.6937 - accuracy: 0.4911 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 852/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6935 - accuracy: 0.4955\n",
      "Epoch 00852: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4956 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 853/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.4600\n",
      "Epoch 00853: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6937 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 854/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4778\n",
      "Epoch 00854: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 855/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6934 - accuracy: 0.4866\n",
      "Epoch 00855: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 856/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4778\n",
      "Epoch 00856: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4778 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 857/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4689\n",
      "Epoch 00857: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4689 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 858/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4911\n",
      "Epoch 00858: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 859/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.5000\n",
      "Epoch 00859: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 860/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.5044\n",
      "Epoch 00860: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6939 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 861/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.4956\n",
      "Epoch 00861: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6943 - accuracy: 0.4956 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 862/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 00862: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6934 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 863/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00863: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 864/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4956\n",
      "Epoch 00864: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6938 - accuracy: 0.4956 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 865/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00865: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 866/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00866: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 867/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00867: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 868/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4956\n",
      "Epoch 00868: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 869/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4644\n",
      "Epoch 00869: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6941 - accuracy: 0.4644 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 870/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4733\n",
      "Epoch 00870: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4733 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 871/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4956\n",
      "Epoch 00871: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4956 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 872/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00872: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 873/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00873: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 874/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00874: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 875/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4556\n",
      "Epoch 00875: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.4556 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 876/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00876: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 877/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4867\n",
      "Epoch 00877: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6938 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 878/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4778\n",
      "Epoch 00878: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6940 - accuracy: 0.4778 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 879/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4556\n",
      "Epoch 00879: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6941 - accuracy: 0.4556 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 880/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4867\n",
      "Epoch 00880: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6933 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 881/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00881: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 882/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00882: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 883/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.6935 - accuracy: 0.4911\n",
      "Epoch 00883: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6935 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 884/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 00884: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 885/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 00885: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 886/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4956\n",
      "Epoch 00886: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6935 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 887/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00887: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 888/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5000\n",
      "Epoch 00888: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 889/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 00889: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 890/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00890: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 891/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 00891: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 892/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.5000\n",
      "Epoch 00892: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6941 - accuracy: 0.5000 - val_loss: 0.6943 - val_accuracy: 0.5000\n",
      "Epoch 893/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6945 - accuracy: 0.4867\n",
      "Epoch 00893: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6945 - accuracy: 0.4867 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 894/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.5000\n",
      "Epoch 00894: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6938 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 895/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.5000\n",
      "Epoch 00895: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.6941 - accuracy: 0.5000 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 896/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6974 - accuracy: 0.5267\n",
      "Epoch 00896: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6974 - accuracy: 0.5267 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 897/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.6970 - accuracy: 0.5089\n",
      "Epoch 00897: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.6970 - accuracy: 0.5089 - val_loss: 0.7093 - val_accuracy: 0.5000\n",
      "Epoch 898/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.1097 - accuracy: 0.4867\n",
      "Epoch 00898: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.1097 - accuracy: 0.4867 - val_loss: 0.7243 - val_accuracy: 0.5000\n",
      "Epoch 899/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.8535 - accuracy: 0.5311\n",
      "Epoch 00899: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 8.8535 - accuracy: 0.5311 - val_loss: 3.3650 - val_accuracy: 0.5000\n",
      "Epoch 900/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.1966 - accuracy: 0.4956\n",
      "Epoch 00900: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 5.1966 - accuracy: 0.4956 - val_loss: 0.7232 - val_accuracy: 0.5000\n",
      "Epoch 901/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 3.2429 - accuracy: 0.5045\n",
      "Epoch 00901: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 3.2473 - accuracy: 0.5044 - val_loss: 4.2269 - val_accuracy: 0.5000\n",
      "Epoch 902/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.6379 - accuracy: 0.5089\n",
      "Epoch 00902: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 7.6379 - accuracy: 0.5089 - val_loss: 4.0922 - val_accuracy: 0.5000\n",
      "Epoch 903/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.0325 - accuracy: 0.5000\n",
      "Epoch 00903: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 4.0325 - accuracy: 0.5000 - val_loss: 3.9669 - val_accuracy: 0.5000\n",
      "Epoch 904/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.9092 - accuracy: 0.5000\n",
      "Epoch 00904: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 3.9092 - accuracy: 0.5000 - val_loss: 3.8533 - val_accuracy: 0.5000\n",
      "Epoch 905/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.8025 - accuracy: 0.5000\n",
      "Epoch 00905: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 3.8025 - accuracy: 0.5000 - val_loss: 3.7511 - val_accuracy: 0.5000\n",
      "Epoch 906/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.7073 - accuracy: 0.5000\n",
      "Epoch 00906: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 3.7073 - accuracy: 0.5000 - val_loss: 3.6578 - val_accuracy: 0.5000\n",
      "Epoch 907/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.6144 - accuracy: 0.5000\n",
      "Epoch 00907: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 3.6144 - accuracy: 0.5000 - val_loss: 3.5716 - val_accuracy: 0.5000\n",
      "Epoch 908/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.5318 - accuracy: 0.5000\n",
      "Epoch 00908: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 3.5318 - accuracy: 0.5000 - val_loss: 3.4912 - val_accuracy: 0.5000\n",
      "Epoch 909/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.4562 - accuracy: 0.5000\n",
      "Epoch 00909: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 3.4562 - accuracy: 0.5000 - val_loss: 3.4155 - val_accuracy: 0.5000\n",
      "Epoch 910/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.3798 - accuracy: 0.5000\n",
      "Epoch 00910: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 3.3798 - accuracy: 0.5000 - val_loss: 3.3439 - val_accuracy: 0.5000\n",
      "Epoch 911/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.3116 - accuracy: 0.5000\n",
      "Epoch 00911: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 3.3116 - accuracy: 0.5000 - val_loss: 3.2759 - val_accuracy: 0.5000\n",
      "Epoch 912/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.2453 - accuracy: 0.5000\n",
      "Epoch 00912: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 3.2453 - accuracy: 0.5000 - val_loss: 3.2109 - val_accuracy: 0.5000\n",
      "Epoch 913/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.1815 - accuracy: 0.5000\n",
      "Epoch 00913: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 3.1815 - accuracy: 0.5000 - val_loss: 3.1488 - val_accuracy: 0.5000\n",
      "Epoch 914/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.1213 - accuracy: 0.5000\n",
      "Epoch 00914: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 3.1213 - accuracy: 0.5000 - val_loss: 3.0892 - val_accuracy: 0.5000\n",
      "Epoch 915/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 3.0641 - accuracy: 0.5000\n",
      "Epoch 00915: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 3.0640 - accuracy: 0.5000 - val_loss: 3.0318 - val_accuracy: 0.5000\n",
      "Epoch 916/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.0049 - accuracy: 0.5000\n",
      "Epoch 00916: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 3.0049 - accuracy: 0.5000 - val_loss: 2.9765 - val_accuracy: 0.5000\n",
      "Epoch 917/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.9527 - accuracy: 0.5000\n",
      "Epoch 00917: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 2.9527 - accuracy: 0.5000 - val_loss: 2.9232 - val_accuracy: 0.5000\n",
      "Epoch 918/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.8990 - accuracy: 0.5000\n",
      "Epoch 00918: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 2.8990 - accuracy: 0.5000 - val_loss: 2.8716 - val_accuracy: 0.5000\n",
      "Epoch 919/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.8493 - accuracy: 0.5000\n",
      "Epoch 00919: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.8493 - accuracy: 0.5000 - val_loss: 2.8217 - val_accuracy: 0.5000\n",
      "Epoch 920/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.8001 - accuracy: 0.5000\n",
      "Epoch 00920: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.8001 - accuracy: 0.5000 - val_loss: 2.7734 - val_accuracy: 0.5000\n",
      "Epoch 921/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.7522 - accuracy: 0.5000\n",
      "Epoch 00921: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 2.7522 - accuracy: 0.5000 - val_loss: 2.7265 - val_accuracy: 0.5000\n",
      "Epoch 922/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.7046 - accuracy: 0.5000\n",
      "Epoch 00922: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 2.7046 - accuracy: 0.5000 - val_loss: 2.6810 - val_accuracy: 0.5000\n",
      "Epoch 923/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.6591 - accuracy: 0.5000\n",
      "Epoch 00923: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 2.6591 - accuracy: 0.5000 - val_loss: 2.6368 - val_accuracy: 0.5000\n",
      "Epoch 924/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.6171 - accuracy: 0.5000\n",
      "Epoch 00924: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 2.6171 - accuracy: 0.5000 - val_loss: 2.5939 - val_accuracy: 0.5000\n",
      "Epoch 925/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.5751 - accuracy: 0.5000\n",
      "Epoch 00925: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 2.5751 - accuracy: 0.5000 - val_loss: 2.5522 - val_accuracy: 0.5000\n",
      "Epoch 926/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.5342 - accuracy: 0.5000\n",
      "Epoch 00926: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 2.5342 - accuracy: 0.5000 - val_loss: 2.5115 - val_accuracy: 0.5000\n",
      "Epoch 927/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.4923 - accuracy: 0.5000\n",
      "Epoch 00927: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 2.4923 - accuracy: 0.5000 - val_loss: 2.4720 - val_accuracy: 0.5000\n",
      "Epoch 928/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.4548 - accuracy: 0.5000\n",
      "Epoch 00928: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 2.4548 - accuracy: 0.5000 - val_loss: 2.4335 - val_accuracy: 0.5000\n",
      "Epoch 929/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.4156 - accuracy: 0.5000\n",
      "Epoch 00929: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.4156 - accuracy: 0.5000 - val_loss: 2.3960 - val_accuracy: 0.5000\n",
      "Epoch 930/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.3796 - accuracy: 0.5000\n",
      "Epoch 00930: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 2.3796 - accuracy: 0.5000 - val_loss: 2.3595 - val_accuracy: 0.5000\n",
      "Epoch 931/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.3436 - accuracy: 0.5000\n",
      "Epoch 00931: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.3436 - accuracy: 0.5000 - val_loss: 2.3239 - val_accuracy: 0.5000\n",
      "Epoch 932/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.3076 - accuracy: 0.5000\n",
      "Epoch 00932: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.3076 - accuracy: 0.5000 - val_loss: 2.2892 - val_accuracy: 0.5000\n",
      "Epoch 933/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.2735 - accuracy: 0.5000\n",
      "Epoch 00933: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.2735 - accuracy: 0.5000 - val_loss: 2.2554 - val_accuracy: 0.5000\n",
      "Epoch 934/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.2392 - accuracy: 0.5000\n",
      "Epoch 00934: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.2392 - accuracy: 0.5000 - val_loss: 2.2224 - val_accuracy: 0.5000\n",
      "Epoch 935/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.2074 - accuracy: 0.5000\n",
      "Epoch 00935: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.2074 - accuracy: 0.5000 - val_loss: 2.1902 - val_accuracy: 0.5000\n",
      "Epoch 936/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.1750 - accuracy: 0.5000\n",
      "Epoch 00936: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.1750 - accuracy: 0.5000 - val_loss: 2.1588 - val_accuracy: 0.5000\n",
      "Epoch 937/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.1437 - accuracy: 0.5000\n",
      "Epoch 00937: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.1437 - accuracy: 0.5000 - val_loss: 2.1281 - val_accuracy: 0.5000\n",
      "Epoch 938/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.1143 - accuracy: 0.5000\n",
      "Epoch 00938: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.1143 - accuracy: 0.5000 - val_loss: 2.0982 - val_accuracy: 0.5000\n",
      "Epoch 939/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.0850 - accuracy: 0.5000\n",
      "Epoch 00939: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.0850 - accuracy: 0.5000 - val_loss: 2.0691 - val_accuracy: 0.5000\n",
      "Epoch 940/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.0562 - accuracy: 0.5000\n",
      "Epoch 00940: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.0562 - accuracy: 0.5000 - val_loss: 2.0406 - val_accuracy: 0.5000\n",
      "Epoch 941/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.0268 - accuracy: 0.5000\n",
      "Epoch 00941: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.0268 - accuracy: 0.5000 - val_loss: 2.0128 - val_accuracy: 0.5000\n",
      "Epoch 942/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 2.0000 - accuracy: 0.5000\n",
      "Epoch 00942: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.0000 - accuracy: 0.5000 - val_loss: 1.9857 - val_accuracy: 0.5000\n",
      "Epoch 943/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.9730 - accuracy: 0.5000\n",
      "Epoch 00943: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.9730 - accuracy: 0.5000 - val_loss: 1.9592 - val_accuracy: 0.5000\n",
      "Epoch 944/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.9464 - accuracy: 0.5000\n",
      "Epoch 00944: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.9464 - accuracy: 0.5000 - val_loss: 1.9334 - val_accuracy: 0.5000\n",
      "Epoch 945/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.9212 - accuracy: 0.5000\n",
      "Epoch 00945: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.9212 - accuracy: 0.5000 - val_loss: 1.9081 - val_accuracy: 0.5000\n",
      "Epoch 946/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.8970 - accuracy: 0.5000\n",
      "Epoch 00946: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.8970 - accuracy: 0.5000 - val_loss: 1.8835 - val_accuracy: 0.5000\n",
      "Epoch 947/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.8719 - accuracy: 0.5000\n",
      "Epoch 00947: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.8719 - accuracy: 0.5000 - val_loss: 1.8594 - val_accuracy: 0.5000\n",
      "Epoch 948/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.8488 - accuracy: 0.5000\n",
      "Epoch 00948: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.8488 - accuracy: 0.5000 - val_loss: 1.8359 - val_accuracy: 0.5000\n",
      "Epoch 949/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.8248 - accuracy: 0.5000\n",
      "Epoch 00949: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.8248 - accuracy: 0.5000 - val_loss: 1.8130 - val_accuracy: 0.5000\n",
      "Epoch 950/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 1.8033 - accuracy: 0.5000\n",
      "Epoch 00950: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.8032 - accuracy: 0.5000 - val_loss: 1.7906 - val_accuracy: 0.5000\n",
      "Epoch 951/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.7800 - accuracy: 0.5000\n",
      "Epoch 00951: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.7800 - accuracy: 0.5000 - val_loss: 1.7688 - val_accuracy: 0.5000\n",
      "Epoch 952/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.7581 - accuracy: 0.5000\n",
      "Epoch 00952: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 1.7581 - accuracy: 0.5000 - val_loss: 1.7474 - val_accuracy: 0.5000\n",
      "Epoch 953/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.7370 - accuracy: 0.5000\n",
      "Epoch 00953: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 1.7370 - accuracy: 0.5000 - val_loss: 1.7266 - val_accuracy: 0.5000\n",
      "Epoch 954/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.7166 - accuracy: 0.5000\n",
      "Epoch 00954: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.7166 - accuracy: 0.5000 - val_loss: 1.7062 - val_accuracy: 0.5000\n",
      "Epoch 955/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 1.6976 - accuracy: 0.5000\n",
      "Epoch 00955: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.6975 - accuracy: 0.5000 - val_loss: 1.6863 - val_accuracy: 0.5000\n",
      "Epoch 956/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.6771 - accuracy: 0.5000\n",
      "Epoch 00956: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.6771 - accuracy: 0.5000 - val_loss: 1.6669 - val_accuracy: 0.5000\n",
      "Epoch 957/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.6577 - accuracy: 0.5000\n",
      "Epoch 00957: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.6577 - accuracy: 0.5000 - val_loss: 1.6480 - val_accuracy: 0.5000\n",
      "Epoch 958/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.6388 - accuracy: 0.5000\n",
      "Epoch 00958: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 1.6388 - accuracy: 0.5000 - val_loss: 1.6294 - val_accuracy: 0.5000\n",
      "Epoch 959/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.6213 - accuracy: 0.5000\n",
      "Epoch 00959: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.6213 - accuracy: 0.5000 - val_loss: 1.6114 - val_accuracy: 0.5000\n",
      "Epoch 960/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.6031 - accuracy: 0.5000\n",
      "Epoch 00960: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.6031 - accuracy: 0.5000 - val_loss: 1.5937 - val_accuracy: 0.5000\n",
      "Epoch 961/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.5855 - accuracy: 0.5000\n",
      "Epoch 00961: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.5855 - accuracy: 0.5000 - val_loss: 1.5764 - val_accuracy: 0.5000\n",
      "Epoch 962/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.5688 - accuracy: 0.5000\n",
      "Epoch 00962: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.5688 - accuracy: 0.5000 - val_loss: 1.5596 - val_accuracy: 0.5000\n",
      "Epoch 963/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.5523 - accuracy: 0.5000\n",
      "Epoch 00963: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.5523 - accuracy: 0.5000 - val_loss: 1.5431 - val_accuracy: 0.5000\n",
      "Epoch 964/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.5360 - accuracy: 0.5000\n",
      "Epoch 00964: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.5360 - accuracy: 0.5000 - val_loss: 1.5271 - val_accuracy: 0.5000\n",
      "Epoch 965/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.5199 - accuracy: 0.5000\n",
      "Epoch 00965: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.5199 - accuracy: 0.5000 - val_loss: 1.5114 - val_accuracy: 0.5000\n",
      "Epoch 966/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.5041 - accuracy: 0.5000\n",
      "Epoch 00966: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.5041 - accuracy: 0.5000 - val_loss: 1.4960 - val_accuracy: 0.5000\n",
      "Epoch 967/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4888 - accuracy: 0.5000\n",
      "Epoch 00967: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.4888 - accuracy: 0.5000 - val_loss: 1.4811 - val_accuracy: 0.5000\n",
      "Epoch 968/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4740 - accuracy: 0.5000\n",
      "Epoch 00968: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.4740 - accuracy: 0.5000 - val_loss: 1.4664 - val_accuracy: 0.5000\n",
      "Epoch 969/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4599 - accuracy: 0.5000\n",
      "Epoch 00969: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.4599 - accuracy: 0.5000 - val_loss: 1.4521 - val_accuracy: 0.5000\n",
      "Epoch 970/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4457 - accuracy: 0.5000\n",
      "Epoch 00970: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.4457 - accuracy: 0.5000 - val_loss: 1.4382 - val_accuracy: 0.5000\n",
      "Epoch 971/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 1.4322 - accuracy: 0.5000\n",
      "Epoch 00971: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.4322 - accuracy: 0.5000 - val_loss: 1.4245 - val_accuracy: 0.5000\n",
      "Epoch 972/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4181 - accuracy: 0.5000\n",
      "Epoch 00972: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.4181 - accuracy: 0.5000 - val_loss: 1.4112 - val_accuracy: 0.5000\n",
      "Epoch 973/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.4049 - accuracy: 0.5000\n",
      "Epoch 00973: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.4049 - accuracy: 0.5000 - val_loss: 1.3982 - val_accuracy: 0.5000\n",
      "Epoch 974/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3922 - accuracy: 0.5000\n",
      "Epoch 00974: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.3922 - accuracy: 0.5000 - val_loss: 1.3854 - val_accuracy: 0.5000\n",
      "Epoch 975/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3798 - accuracy: 0.5000\n",
      "Epoch 00975: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.3798 - accuracy: 0.5000 - val_loss: 1.3730 - val_accuracy: 0.5000\n",
      "Epoch 976/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3673 - accuracy: 0.5000\n",
      "Epoch 00976: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.3673 - accuracy: 0.5000 - val_loss: 1.3608 - val_accuracy: 0.5000\n",
      "Epoch 977/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3554 - accuracy: 0.5000\n",
      "Epoch 00977: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.3554 - accuracy: 0.5000 - val_loss: 1.3490 - val_accuracy: 0.5000\n",
      "Epoch 978/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3435 - accuracy: 0.5000\n",
      "Epoch 00978: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.3435 - accuracy: 0.5000 - val_loss: 1.3374 - val_accuracy: 0.5000\n",
      "Epoch 979/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3318 - accuracy: 0.5000\n",
      "Epoch 00979: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.3318 - accuracy: 0.5000 - val_loss: 1.3260 - val_accuracy: 0.5000\n",
      "Epoch 980/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 1.3212 - accuracy: 0.5000\n",
      "Epoch 00980: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.3212 - accuracy: 0.5000 - val_loss: 1.3149 - val_accuracy: 0.5000\n",
      "Epoch 981/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.3098 - accuracy: 0.5000\n",
      "Epoch 00981: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.3098 - accuracy: 0.5000 - val_loss: 1.3041 - val_accuracy: 0.5000\n",
      "Epoch 982/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.2991 - accuracy: 0.5000\n",
      "Epoch 00982: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.2991 - accuracy: 0.5000 - val_loss: 1.2935 - val_accuracy: 0.5000\n",
      "Epoch 983/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.2886 - accuracy: 0.5000\n",
      "Epoch 00983: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.2886 - accuracy: 0.5000 - val_loss: 1.2831 - val_accuracy: 0.5000\n",
      "Epoch 984/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.2785 - accuracy: 0.5000\n",
      "Epoch 00984: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 1.2785 - accuracy: 0.5000 - val_loss: 1.2730 - val_accuracy: 0.5000\n",
      "Epoch 985/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.2681 - accuracy: 0.5000\n",
      "Epoch 00985: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.2681 - accuracy: 0.5000 - val_loss: 1.2631 - val_accuracy: 0.5000\n",
      "Epoch 986/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.2583 - accuracy: 0.5000\n",
      "Epoch 00986: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.2583 - accuracy: 0.5000 - val_loss: 1.2534 - val_accuracy: 0.5000\n",
      "Epoch 987/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.2492 - accuracy: 0.5000\n",
      "Epoch 00987: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.2492 - accuracy: 0.5000 - val_loss: 1.2439 - val_accuracy: 0.5000\n",
      "Epoch 988/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 1.2399 - accuracy: 0.5000\n",
      "Epoch 00988: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.2398 - accuracy: 0.5000 - val_loss: 1.2346 - val_accuracy: 0.5000\n",
      "Epoch 989/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.2304 - accuracy: 0.5000\n",
      "Epoch 00989: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.2304 - accuracy: 0.5000 - val_loss: 1.2256 - val_accuracy: 0.5000\n",
      "Epoch 990/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.2212 - accuracy: 0.5000\n",
      "Epoch 00990: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.2212 - accuracy: 0.5000 - val_loss: 1.2167 - val_accuracy: 0.5000\n",
      "Epoch 991/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.2124 - accuracy: 0.5000\n",
      "Epoch 00991: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.2124 - accuracy: 0.5000 - val_loss: 1.2080 - val_accuracy: 0.5000\n",
      "Epoch 992/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.2038 - accuracy: 0.5000\n",
      "Epoch 00992: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 1.2038 - accuracy: 0.5000 - val_loss: 1.1995 - val_accuracy: 0.5000\n",
      "Epoch 993/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.1955 - accuracy: 0.5000\n",
      "Epoch 00993: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.1955 - accuracy: 0.5000 - val_loss: 1.1912 - val_accuracy: 0.5000\n",
      "Epoch 994/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.1875 - accuracy: 0.5000\n",
      "Epoch 00994: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.1875 - accuracy: 0.5000 - val_loss: 1.1831 - val_accuracy: 0.5000\n",
      "Epoch 995/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.1792 - accuracy: 0.5000\n",
      "Epoch 00995: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 1.1792 - accuracy: 0.5000 - val_loss: 1.1751 - val_accuracy: 0.5000\n",
      "Epoch 996/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.1716 - accuracy: 0.5000\n",
      "Epoch 00996: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 1.1716 - accuracy: 0.5000 - val_loss: 1.1674 - val_accuracy: 0.5000\n",
      "Epoch 997/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.1639 - accuracy: 0.5000\n",
      "Epoch 00997: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.1639 - accuracy: 0.5000 - val_loss: 1.1597 - val_accuracy: 0.5000\n",
      "Epoch 998/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.1562 - accuracy: 0.5000\n",
      "Epoch 00998: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.1562 - accuracy: 0.5000 - val_loss: 1.1523 - val_accuracy: 0.5000\n",
      "Epoch 999/1000\n",
      "15/15 [==============================] - ETA: 0s - loss: 1.1487 - accuracy: 0.5000\n",
      "Epoch 00999: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.1487 - accuracy: 0.5000 - val_loss: 1.1450 - val_accuracy: 0.5000\n",
      "Epoch 1000/1000\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 1.1419 - accuracy: 0.5000\n",
      "Epoch 01000: val_accuracy did not improve from 0.50000\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.1419 - accuracy: 0.5000 - val_loss: 1.1378 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "hist_model = model.fit(train_dataset,\n",
    "                       epochs = epochs,\n",
    "                       steps_per_epoch=len(train_dataset),\n",
    "                       validation_data=validation_dataset,\n",
    "                       \n",
    "                       callbacks=[tensorboard_callback(\"Tensorboard\",\"model\"),                                  model_checkpoint(\"Checkpoints\",\"model_\"+str(epochs)+\".ckpt\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8d85291-ac5e-49b5-8144-59b7cb40effd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  237\n"
     ]
    }
   ],
   "source": [
    "base_model.trainable = True\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 50\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc34bf4-08f4-4ca5-8c7e-16deaaa87190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
